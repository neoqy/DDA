{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp, sqrt\n",
    "from torch.autograd import Variable\n",
    "from my_dataset import MNIST_M\n",
    "from my_dataset import ST_Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension\n",
    "\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "from data_loader import get_train_test_loader, get_office31_dataloader\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))\n",
    "\n",
    "def reset_seq(seq):\n",
    "    for m in seq:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, sqrt(2 / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            size = m.weight.size()\n",
    "            fan_out = size[0] # number of rows\n",
    "            fan_in = size[1] # number of columns\n",
    "            m.weight.data.normal_(0, sqrt(2 / (fan_in + fan_out)))\n",
    "            m.bias.data.zero_()\n",
    "        elif hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()\n",
    "            \n",
    "def evaluate_da_accuracy(model, dataloader, source):\n",
    "    model.eval()\n",
    "    correct_LC = 0\n",
    "    correct_DC = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            if (use_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs_LC, outputs_DC = model(inputs)\n",
    "            correct_LC += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "            if source:\n",
    "                correct_DC += labels.size()[0] - outputs_DC.data.sum().item()\n",
    "            else:\n",
    "                correct_DC += outputs_DC.data.sum().item()\n",
    "            total += labels.size()[0]\n",
    "        acc_LC = correct_LC / total\n",
    "        acc_DC = correct_DC / total\n",
    "    return acc_LC, acc_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets: amazon\n"
     ]
    }
   ],
   "source": [
    "trainloader_source = get_office31_dataloader(\"amazon\", batch_size=102)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets: webcam\n"
     ]
    }
   ],
   "source": [
    "trainloader_target = get_office31_dataloader(\"webcam\", batch_size=26)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, lamda):\n",
    "        ctx.save_for_backward(lamda)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        lamda, = ctx.saved_tensors\n",
    "        return -lamda * grad_outputs, None\n",
    "\n",
    "class GRL(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init):\n",
    "        super(GRL, self).__init__()\n",
    "        self.GRL_func = GRL_func.apply\n",
    "        self.lamda = nn.Parameter(torch.Tensor(1), requires_grad=False)\n",
    "        self.set_lamda(lamda_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.GRL_func(x, self.lamda)\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.lamda[0] = lamda_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_DA(nn.Module):\n",
    "\n",
    "    def __init__(self, lamda_init):\n",
    "        super(AlexNet_DA, self).__init__()\n",
    "        # lambda\n",
    "        self.lamda = lamda_init\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 31)\n",
    "        )\n",
    "        self.GRL_layer = GRL(lamda_init)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        x_l = self.label_classifier(x)\n",
    "        x_d = self.GRL_layer(x)\n",
    "        x_d = self.domain_classifier(x_d)\n",
    "        return x_l, x_d\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.GRL_layer.set_lamda(lamda_new)\n",
    "    \n",
    "    def load_pretrained_part(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                print(name)\n",
    "                continue\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classifier.6.weight\n",
      "classifier.6.bias\n",
      "AlexNet_DA(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Dropout(p=0.5)\n",
      "    (7): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (label_classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      "  (GRL_layer): GRL()\n",
      "  (domain_classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_da = AlexNet_DA(0)\n",
    "\n",
    "model_url = \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\"\n",
    "\n",
    "cnn_da.load_pretrained_part(model_zoo.load_url(model_url))\n",
    "\n",
    "if (use_gpu):\n",
    "    cnn_da.cuda()\n",
    "print(cnn_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in cnn_da.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 6.873\n",
      "[ 4] batch loss: 5.648\n",
      "[ 6] batch loss: 4.102\n",
      "[ 8] batch loss: 3.550\n",
      "[10] batch loss: 3.385\n",
      "[12] batch loss: 3.214\n",
      "[14] batch loss: 3.267\n",
      "[16] batch loss: 3.199\n",
      "[18] batch loss: 3.106\n",
      "[20] batch loss: 3.185\n",
      "[22] batch loss: 2.953\n",
      "[24] batch loss: 2.974\n",
      "[26] batch loss: 2.832\n",
      "[28] batch loss: 2.536\n",
      "epoch 1 loss: inf -> 101.650\n",
      "\n",
      "[ 2] batch loss: 2.573\n",
      "[ 4] batch loss: 2.658\n",
      "[ 6] batch loss: 2.388\n",
      "[ 8] batch loss: 2.459\n",
      "[10] batch loss: 2.232\n",
      "[12] batch loss: 2.156\n",
      "[14] batch loss: 2.064\n",
      "[16] batch loss: 2.219\n",
      "[18] batch loss: 2.023\n",
      "[20] batch loss: 1.902\n",
      "[22] batch loss: 1.771\n",
      "[24] batch loss: 1.908\n",
      "[26] batch loss: 1.924\n",
      "[28] batch loss: 1.637\n",
      "epoch 2 loss: 101.650 -> 59.829\n",
      "\n",
      "[ 2] batch loss: 1.819\n",
      "[ 4] batch loss: 1.434\n",
      "[ 6] batch loss: 1.689\n",
      "[ 8] batch loss: 1.657\n",
      "[10] batch loss: 1.607\n",
      "[12] batch loss: 1.493\n",
      "[14] batch loss: 1.542\n",
      "[16] batch loss: 1.434\n",
      "[18] batch loss: 1.370\n",
      "[20] batch loss: 1.589\n",
      "[22] batch loss: 1.226\n",
      "[24] batch loss: 1.169\n",
      "[26] batch loss: 1.416\n",
      "[28] batch loss: 1.523\n",
      "epoch 3 loss: 59.829 -> 41.937\n",
      "\n",
      "[ 2] batch loss: 1.309\n",
      "[ 4] batch loss: 1.137\n",
      "[ 6] batch loss: 1.205\n",
      "[ 8] batch loss: 1.262\n",
      "[10] batch loss: 1.076\n",
      "[12] batch loss: 1.361\n",
      "[14] batch loss: 1.070\n",
      "[16] batch loss: 1.124\n",
      "[18] batch loss: 1.219\n",
      "[20] batch loss: 1.350\n",
      "[22] batch loss: 1.081\n",
      "[24] batch loss: 1.233\n",
      "[26] batch loss: 1.013\n",
      "[28] batch loss: 1.002\n",
      "epoch 4 loss: 41.937 -> 32.883\n",
      "\n",
      "[ 2] batch loss: 1.187\n",
      "[ 4] batch loss: 1.213\n",
      "[ 6] batch loss: 1.017\n",
      "[ 8] batch loss: 0.914\n",
      "[10] batch loss: 1.014\n",
      "[12] batch loss: 0.861\n",
      "[14] batch loss: 1.055\n",
      "[16] batch loss: 0.980\n",
      "[18] batch loss: 0.943\n",
      "[20] batch loss: 1.263\n",
      "[22] batch loss: 0.982\n",
      "[24] batch loss: 0.955\n",
      "[26] batch loss: 0.969\n",
      "[28] batch loss: 1.125\n",
      "epoch 5 loss: 32.883 -> 28.959\n",
      "\n",
      "[ 2] batch loss: 0.884\n",
      "[ 4] batch loss: 0.907\n",
      "[ 6] batch loss: 0.953\n",
      "[ 8] batch loss: 0.890\n",
      "[10] batch loss: 0.990\n",
      "[12] batch loss: 0.808\n",
      "[14] batch loss: 0.960\n",
      "[16] batch loss: 0.951\n",
      "[18] batch loss: 1.058\n",
      "[20] batch loss: 0.838\n",
      "[22] batch loss: 0.970\n",
      "[24] batch loss: 0.803\n",
      "[26] batch loss: 0.994\n",
      "[28] batch loss: 0.824\n",
      "epoch 6 loss: 28.959 -> 25.659\n",
      "\n",
      "[ 2] batch loss: 0.773\n",
      "[ 4] batch loss: 0.844\n",
      "[ 6] batch loss: 0.907\n",
      "[ 8] batch loss: 0.599\n",
      "[10] batch loss: 0.759\n",
      "[12] batch loss: 0.872\n",
      "[14] batch loss: 0.774\n",
      "[16] batch loss: 0.790\n",
      "[18] batch loss: 0.903\n",
      "[20] batch loss: 0.708\n",
      "[22] batch loss: 0.705\n",
      "[24] batch loss: 0.773\n",
      "[26] batch loss: 0.786\n",
      "[28] batch loss: 0.768\n",
      "epoch 7 loss: 25.659 -> 21.922\n",
      "\n",
      "[ 2] batch loss: 0.860\n",
      "[ 4] batch loss: 0.692\n",
      "[ 6] batch loss: 0.602\n",
      "[ 8] batch loss: 0.697\n",
      "[10] batch loss: 0.847\n",
      "[12] batch loss: 0.732\n",
      "[14] batch loss: 0.664\n",
      "[16] batch loss: 0.652\n",
      "[18] batch loss: 0.671\n",
      "[20] batch loss: 0.765\n",
      "[22] batch loss: 0.699\n",
      "[24] batch loss: 0.685\n",
      "[26] batch loss: 0.729\n",
      "[28] batch loss: 0.719\n",
      "epoch 8 loss: 21.922 -> 20.026\n",
      "\n",
      "[ 2] batch loss: 0.691\n",
      "[ 4] batch loss: 0.700\n",
      "[ 6] batch loss: 0.799\n",
      "[ 8] batch loss: 0.724\n",
      "[10] batch loss: 0.566\n",
      "[12] batch loss: 0.707\n",
      "[14] batch loss: 0.626\n",
      "[16] batch loss: 0.733\n",
      "[18] batch loss: 0.589\n",
      "[20] batch loss: 0.691\n",
      "[22] batch loss: 0.536\n",
      "[24] batch loss: 0.626\n",
      "[26] batch loss: 0.642\n",
      "[28] batch loss: 0.734\n",
      "epoch 9 loss: 20.026 -> 18.724\n",
      "\n",
      "[ 2] batch loss: 0.644\n",
      "[ 4] batch loss: 0.569\n",
      "[ 6] batch loss: 0.627\n",
      "[ 8] batch loss: 0.586\n",
      "[10] batch loss: 0.569\n",
      "[12] batch loss: 0.450\n",
      "[14] batch loss: 0.371\n",
      "[16] batch loss: 0.550\n",
      "[18] batch loss: 0.612\n",
      "[20] batch loss: 0.466\n",
      "[22] batch loss: 0.517\n",
      "[24] batch loss: 0.530\n",
      "[26] batch loss: 0.664\n",
      "[28] batch loss: 0.421\n",
      "epoch 10 loss: 18.724 -> 15.154\n",
      "\n",
      "[ 2] batch loss: 0.511\n",
      "[ 4] batch loss: 0.562\n",
      "[ 6] batch loss: 0.571\n",
      "[ 8] batch loss: 0.484\n",
      "[10] batch loss: 0.559\n",
      "[12] batch loss: 0.464\n",
      "[14] batch loss: 0.417\n",
      "[16] batch loss: 0.515\n",
      "[18] batch loss: 0.538\n",
      "[20] batch loss: 0.595\n",
      "[22] batch loss: 0.481\n",
      "[24] batch loss: 0.614\n",
      "[26] batch loss: 0.732\n",
      "[28] batch loss: 0.469\n",
      "epoch 11 loss: 15.154 -> 15.023\n",
      "\n",
      "[ 2] batch loss: 0.494\n",
      "[ 4] batch loss: 0.532\n",
      "[ 6] batch loss: 0.399\n",
      "[ 8] batch loss: 0.558\n",
      "[10] batch loss: 0.439\n",
      "[12] batch loss: 0.495\n",
      "[14] batch loss: 0.454\n",
      "[16] batch loss: 0.611\n",
      "[18] batch loss: 0.382\n",
      "[20] batch loss: 0.373\n",
      "[22] batch loss: 0.421\n",
      "[24] batch loss: 0.544\n",
      "[26] batch loss: 0.567\n",
      "[28] batch loss: 0.469\n",
      "epoch 12 loss: 15.023 -> 13.477\n",
      "\n",
      "[ 2] batch loss: 0.503\n",
      "[ 4] batch loss: 0.396\n",
      "[ 6] batch loss: 0.431\n",
      "[ 8] batch loss: 0.430\n",
      "[10] batch loss: 0.444\n",
      "[12] batch loss: 0.400\n",
      "[14] batch loss: 0.386\n",
      "[16] batch loss: 0.646\n",
      "[18] batch loss: 0.395\n",
      "[20] batch loss: 0.364\n",
      "[22] batch loss: 0.385\n",
      "[24] batch loss: 0.492\n",
      "[26] batch loss: 0.525\n",
      "[28] batch loss: 0.541\n",
      "epoch 13 loss: 13.477 -> 12.680\n",
      "\n",
      "[ 2] batch loss: 0.402\n",
      "[ 4] batch loss: 0.337\n",
      "[ 6] batch loss: 0.478\n",
      "[ 8] batch loss: 0.428\n",
      "[10] batch loss: 0.420\n",
      "[12] batch loss: 0.526\n",
      "[14] batch loss: 0.402\n",
      "[16] batch loss: 0.313\n",
      "[18] batch loss: 0.421\n",
      "[20] batch loss: 0.444\n",
      "[22] batch loss: 0.475\n",
      "[24] batch loss: 0.488\n",
      "[26] batch loss: 0.368\n",
      "[28] batch loss: 0.369\n",
      "epoch 14 loss: 12.680 -> 11.741\n",
      "\n",
      "[ 2] batch loss: 0.402\n",
      "[ 4] batch loss: 0.392\n",
      "[ 6] batch loss: 0.437\n",
      "[ 8] batch loss: 0.536\n",
      "[10] batch loss: 0.379\n",
      "[12] batch loss: 0.357\n",
      "[14] batch loss: 0.459\n",
      "[16] batch loss: 0.388\n",
      "[18] batch loss: 0.292\n",
      "[20] batch loss: 0.452\n",
      "[22] batch loss: 0.324\n",
      "[24] batch loss: 0.273\n",
      "[26] batch loss: 0.390\n",
      "[28] batch loss: 0.524\n",
      "epoch 15 loss: 11.741 -> 11.205\n",
      "\n",
      "[ 2] batch loss: 0.411\n",
      "[ 4] batch loss: 0.317\n",
      "[ 6] batch loss: 0.349\n",
      "[ 8] batch loss: 0.357\n",
      "[10] batch loss: 0.376\n",
      "[12] batch loss: 0.354\n",
      "[14] batch loss: 0.419\n",
      "[16] batch loss: 0.339\n",
      "[18] batch loss: 0.367\n",
      "[20] batch loss: 0.410\n",
      "[22] batch loss: 0.311\n",
      "[24] batch loss: 0.342\n",
      "[26] batch loss: 0.329\n",
      "[28] batch loss: 0.330\n",
      "epoch 16 loss: 11.205 -> 10.020\n",
      "\n",
      "[ 2] batch loss: 0.389\n",
      "[ 4] batch loss: 0.317\n",
      "[ 6] batch loss: 0.373\n",
      "[ 8] batch loss: 0.375\n",
      "[10] batch loss: 0.306\n",
      "[12] batch loss: 0.386\n",
      "[14] batch loss: 0.303\n",
      "[16] batch loss: 0.300\n",
      "[18] batch loss: 0.303\n",
      "[20] batch loss: 0.327\n",
      "[22] batch loss: 0.238\n",
      "[24] batch loss: 0.367\n",
      "[26] batch loss: 0.303\n",
      "[28] batch loss: 0.390\n",
      "epoch 17 loss: 10.020 -> 9.355\n",
      "\n",
      "[ 2] batch loss: 0.356\n",
      "[ 4] batch loss: 0.314\n",
      "[ 6] batch loss: 0.291\n",
      "[ 8] batch loss: 0.269\n",
      "[10] batch loss: 0.348\n",
      "[12] batch loss: 0.309\n",
      "[14] batch loss: 0.266\n",
      "[16] batch loss: 0.317\n",
      "[18] batch loss: 0.403\n",
      "[20] batch loss: 0.333\n",
      "[22] batch loss: 0.249\n",
      "[24] batch loss: 0.241\n",
      "[26] batch loss: 0.247\n",
      "[28] batch loss: 0.213\n",
      "epoch 18 loss: 9.355 -> 8.317\n",
      "\n",
      "[ 2] batch loss: 0.309\n",
      "[ 4] batch loss: 0.301\n",
      "[ 6] batch loss: 0.321\n",
      "[ 8] batch loss: 0.355\n",
      "[10] batch loss: 0.295\n",
      "[12] batch loss: 0.325\n",
      "[14] batch loss: 0.240\n",
      "[16] batch loss: 0.221\n",
      "[18] batch loss: 0.233\n",
      "[20] batch loss: 0.302\n",
      "[22] batch loss: 0.256\n",
      "[24] batch loss: 0.326\n",
      "[26] batch loss: 0.315\n",
      "[28] batch loss: 0.392\n",
      "epoch 19 loss: 8.317 -> 8.382\n",
      "\n",
      "[ 2] batch loss: 0.208\n",
      "[ 4] batch loss: 0.250\n",
      "[ 6] batch loss: 0.257\n",
      "[ 8] batch loss: 0.212\n",
      "[10] batch loss: 0.270\n",
      "[12] batch loss: 0.263\n",
      "[14] batch loss: 0.415\n",
      "[16] batch loss: 0.300\n",
      "[18] batch loss: 0.222\n",
      "[20] batch loss: 0.186\n",
      "[22] batch loss: 0.290\n",
      "[24] batch loss: 0.273\n",
      "[26] batch loss: 0.260\n",
      "[28] batch loss: 0.202\n",
      "epoch 20 loss: 8.382 -> 7.215\n",
      "\n",
      "[ 2] batch loss: 0.212\n",
      "[ 4] batch loss: 0.266\n",
      "[ 6] batch loss: 0.361\n",
      "[ 8] batch loss: 0.280\n",
      "[10] batch loss: 0.233\n",
      "[12] batch loss: 0.456\n",
      "[14] batch loss: 0.311\n",
      "[16] batch loss: 0.428\n",
      "[18] batch loss: 0.265\n",
      "[20] batch loss: 0.223\n",
      "[22] batch loss: 0.311\n",
      "[24] batch loss: 0.262\n",
      "[26] batch loss: 0.255\n",
      "[28] batch loss: 0.338\n",
      "epoch 21 loss: 7.215 -> 8.400\n",
      "\n",
      "[ 2] batch loss: 0.348\n",
      "[ 4] batch loss: 0.249\n",
      "[ 6] batch loss: 0.228\n",
      "[ 8] batch loss: 0.266\n",
      "[10] batch loss: 0.366\n",
      "[12] batch loss: 0.187\n",
      "[14] batch loss: 0.207\n",
      "[16] batch loss: 0.268\n",
      "[18] batch loss: 0.219\n",
      "[20] batch loss: 0.324\n",
      "[22] batch loss: 0.288\n",
      "[24] batch loss: 0.273\n",
      "[26] batch loss: 0.326\n",
      "[28] batch loss: 0.321\n",
      "epoch 22 loss: 8.400 -> 7.745\n",
      "\n",
      "[ 2] batch loss: 0.228\n",
      "[ 4] batch loss: 0.252\n",
      "[ 6] batch loss: 0.151\n",
      "[ 8] batch loss: 0.219\n",
      "[10] batch loss: 0.251\n",
      "[12] batch loss: 0.191\n",
      "[14] batch loss: 0.193\n",
      "[16] batch loss: 0.235\n",
      "[18] batch loss: 0.331\n",
      "[20] batch loss: 0.302\n",
      "[22] batch loss: 0.327\n",
      "[24] batch loss: 0.358\n",
      "[26] batch loss: 0.266\n",
      "[28] batch loss: 0.316\n",
      "epoch 23 loss: 7.745 -> 7.238\n",
      "\n",
      "[ 2] batch loss: 0.193\n",
      "[ 4] batch loss: 0.197\n",
      "[ 6] batch loss: 0.297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 8] batch loss: 0.295\n",
      "[10] batch loss: 0.204\n",
      "[12] batch loss: 0.201\n",
      "[14] batch loss: 0.187\n",
      "[16] batch loss: 0.203\n",
      "[18] batch loss: 0.210\n",
      "[20] batch loss: 0.235\n",
      "[22] batch loss: 0.225\n",
      "[24] batch loss: 0.293\n",
      "[26] batch loss: 0.197\n",
      "[28] batch loss: 0.226\n",
      "epoch 24 loss: 7.238 -> 6.328\n",
      "\n",
      "[ 2] batch loss: 0.277\n",
      "[ 4] batch loss: 0.208\n",
      "[ 6] batch loss: 0.246\n",
      "[ 8] batch loss: 0.192\n",
      "[10] batch loss: 0.214\n",
      "[12] batch loss: 0.205\n",
      "[14] batch loss: 0.222\n",
      "[16] batch loss: 0.191\n",
      "[18] batch loss: 0.141\n",
      "[20] batch loss: 0.146\n",
      "[22] batch loss: 0.199\n",
      "[24] batch loss: 0.168\n",
      "[26] batch loss: 0.301\n",
      "[28] batch loss: 0.218\n",
      "epoch 25 loss: 6.328 -> 5.859\n",
      "\n",
      "[ 2] batch loss: 0.183\n",
      "[ 4] batch loss: 0.097\n",
      "[ 6] batch loss: 0.167\n",
      "[ 8] batch loss: 0.098\n",
      "[10] batch loss: 0.175\n",
      "[12] batch loss: 0.143\n",
      "[14] batch loss: 0.271\n",
      "[16] batch loss: 0.153\n",
      "[18] batch loss: 0.171\n",
      "[20] batch loss: 0.195\n",
      "[22] batch loss: 0.190\n",
      "[24] batch loss: 0.148\n",
      "[26] batch loss: 0.148\n",
      "[28] batch loss: 0.172\n",
      "epoch 26 loss: 5.859 -> 4.621\n",
      "\n",
      "[ 2] batch loss: 0.125\n",
      "[ 4] batch loss: 0.116\n",
      "[ 6] batch loss: 0.120\n",
      "[ 8] batch loss: 0.151\n",
      "[10] batch loss: 0.221\n",
      "[12] batch loss: 0.080\n",
      "[14] batch loss: 0.196\n",
      "[16] batch loss: 0.179\n",
      "[18] batch loss: 0.137\n",
      "[20] batch loss: 0.167\n",
      "[22] batch loss: 0.093\n",
      "[24] batch loss: 0.166\n",
      "[26] batch loss: 0.150\n",
      "[28] batch loss: 0.130\n",
      "epoch 27 loss: 4.621 -> 4.062\n",
      "\n",
      "[ 2] batch loss: 0.167\n",
      "[ 4] batch loss: 0.092\n",
      "[ 6] batch loss: 0.075\n",
      "[ 8] batch loss: 0.118\n",
      "[10] batch loss: 0.167\n",
      "[12] batch loss: 0.178\n",
      "[14] batch loss: 0.110\n",
      "[16] batch loss: 0.115\n",
      "[18] batch loss: 0.112\n",
      "[20] batch loss: 0.119\n",
      "[22] batch loss: 0.178\n",
      "[24] batch loss: 0.105\n",
      "[26] batch loss: 0.134\n",
      "[28] batch loss: 0.085\n",
      "epoch 28 loss: 4.062 -> 3.508\n",
      "\n",
      "[ 2] batch loss: 0.093\n",
      "[ 4] batch loss: 0.119\n",
      "[ 6] batch loss: 0.121\n",
      "[ 8] batch loss: 0.136\n",
      "[10] batch loss: 0.103\n",
      "[12] batch loss: 0.109\n",
      "[14] batch loss: 0.084\n",
      "[16] batch loss: 0.141\n",
      "[18] batch loss: 0.116\n",
      "[20] batch loss: 0.189\n",
      "[22] batch loss: 0.068\n",
      "[24] batch loss: 0.097\n",
      "[26] batch loss: 0.087\n",
      "[28] batch loss: 0.136\n",
      "epoch 29 loss: 3.508 -> 3.200\n",
      "\n",
      "[ 2] batch loss: 0.103\n",
      "[ 4] batch loss: 0.102\n",
      "[ 6] batch loss: 0.094\n",
      "[ 8] batch loss: 0.126\n",
      "[10] batch loss: 0.148\n",
      "[12] batch loss: 0.094\n",
      "[14] batch loss: 0.114\n",
      "[16] batch loss: 0.103\n",
      "[18] batch loss: 0.125\n",
      "[20] batch loss: 0.077\n",
      "[22] batch loss: 0.115\n",
      "[24] batch loss: 0.106\n",
      "[26] batch loss: 0.125\n",
      "[28] batch loss: 0.135\n",
      "epoch 30 loss: 3.200 -> 3.136\n",
      "\n",
      "[ 2] batch loss: 0.162\n",
      "[ 4] batch loss: 0.088\n",
      "[ 6] batch loss: 0.111\n",
      "[ 8] batch loss: 0.059\n",
      "[10] batch loss: 0.116\n",
      "[12] batch loss: 0.105\n",
      "[14] batch loss: 0.113\n",
      "[16] batch loss: 0.117\n",
      "[18] batch loss: 0.127\n",
      "[20] batch loss: 0.136\n",
      "[22] batch loss: 0.088\n",
      "[24] batch loss: 0.114\n",
      "[26] batch loss: 0.109\n",
      "[28] batch loss: 0.133\n",
      "epoch 31 loss: 3.136 -> 3.151\n",
      "\n",
      "[ 2] batch loss: 0.102\n",
      "[ 4] batch loss: 0.088\n",
      "[ 6] batch loss: 0.106\n",
      "[ 8] batch loss: 0.132\n",
      "[10] batch loss: 0.085\n",
      "[12] batch loss: 0.078\n",
      "[14] batch loss: 0.076\n",
      "[16] batch loss: 0.082\n",
      "[18] batch loss: 0.099\n",
      "[20] batch loss: 0.121\n",
      "[22] batch loss: 0.163\n",
      "[24] batch loss: 0.062\n",
      "[26] batch loss: 0.104\n",
      "[28] batch loss: 0.055\n",
      "epoch 32 loss: 3.151 -> 2.705\n",
      "\n",
      "[ 2] batch loss: 0.087\n",
      "[ 4] batch loss: 0.094\n",
      "[ 6] batch loss: 0.070\n",
      "[ 8] batch loss: 0.123\n",
      "[10] batch loss: 0.112\n",
      "[12] batch loss: 0.035\n",
      "[14] batch loss: 0.052\n",
      "[16] batch loss: 0.123\n",
      "[18] batch loss: 0.093\n",
      "[20] batch loss: 0.081\n",
      "[22] batch loss: 0.074\n",
      "[24] batch loss: 0.088\n",
      "[26] batch loss: 0.166\n",
      "[28] batch loss: 0.034\n",
      "epoch 33 loss: 2.705 -> 2.463\n",
      "\n",
      "[ 2] batch loss: 0.068\n",
      "[ 4] batch loss: 0.074\n",
      "[ 6] batch loss: 0.106\n",
      "[ 8] batch loss: 0.057\n",
      "[10] batch loss: 0.079\n",
      "[12] batch loss: 0.045\n",
      "[14] batch loss: 0.027\n",
      "[16] batch loss: 0.036\n",
      "[18] batch loss: 0.069\n",
      "[20] batch loss: 0.124\n",
      "[22] batch loss: 0.089\n",
      "[24] batch loss: 0.081\n",
      "[26] batch loss: 0.090\n",
      "[28] batch loss: 0.055\n",
      "epoch 34 loss: 2.463 -> 1.997\n",
      "\n",
      "[ 2] batch loss: 0.087\n",
      "[ 4] batch loss: 0.057\n",
      "[ 6] batch loss: 0.041\n",
      "[ 8] batch loss: 0.051\n",
      "[10] batch loss: 0.061\n",
      "[12] batch loss: 0.100\n",
      "[14] batch loss: 0.079\n",
      "[16] batch loss: 0.066\n",
      "[18] batch loss: 0.090\n",
      "[20] batch loss: 0.085\n",
      "[22] batch loss: 0.044\n",
      "[24] batch loss: 0.065\n",
      "[26] batch loss: 0.098\n",
      "[28] batch loss: 0.079\n",
      "epoch 35 loss: 1.997 -> 2.009\n",
      "\n",
      "[ 2] batch loss: 0.035\n",
      "[ 4] batch loss: 0.108\n",
      "[ 6] batch loss: 0.084\n",
      "[ 8] batch loss: 0.063\n",
      "[10] batch loss: 0.036\n",
      "[12] batch loss: 0.064\n",
      "[14] batch loss: 0.036\n",
      "[16] batch loss: 0.140\n",
      "[18] batch loss: 0.099\n",
      "[20] batch loss: 0.054\n",
      "[22] batch loss: 0.051\n",
      "[24] batch loss: 0.062\n",
      "[26] batch loss: 0.086\n",
      "[28] batch loss: 0.038\n",
      "epoch 36 loss: 2.009 -> 1.911\n",
      "\n",
      "[ 2] batch loss: 0.042\n",
      "[ 4] batch loss: 0.065\n",
      "[ 6] batch loss: 0.060\n",
      "[ 8] batch loss: 0.101\n",
      "[10] batch loss: 0.027\n",
      "[12] batch loss: 0.093\n",
      "[14] batch loss: 0.054\n",
      "[16] batch loss: 0.069\n",
      "[18] batch loss: 0.083\n",
      "[20] batch loss: 0.047\n",
      "[22] batch loss: 0.039\n",
      "[24] batch loss: 0.065\n",
      "[26] batch loss: 0.061\n",
      "[28] batch loss: 0.047\n",
      "epoch 37 loss: 1.911 -> 1.708\n",
      "\n",
      "[ 2] batch loss: 0.104\n",
      "[ 4] batch loss: 0.049\n",
      "[ 6] batch loss: 0.074\n",
      "[ 8] batch loss: 0.100\n",
      "[10] batch loss: 0.067\n",
      "[12] batch loss: 0.037\n",
      "[14] batch loss: 0.071\n",
      "[16] batch loss: 0.078\n",
      "[18] batch loss: 0.119\n",
      "[20] batch loss: 0.052\n",
      "[22] batch loss: 0.069\n",
      "[24] batch loss: 0.063\n",
      "[26] batch loss: 0.060\n",
      "[28] batch loss: 0.076\n",
      "epoch 38 loss: 1.708 -> 2.038\n",
      "\n",
      "[ 2] batch loss: 0.043\n",
      "[ 4] batch loss: 0.094\n",
      "[ 6] batch loss: 0.124\n",
      "[ 8] batch loss: 0.078\n",
      "[10] batch loss: 0.029\n",
      "[12] batch loss: 0.060\n",
      "[14] batch loss: 0.074\n",
      "[16] batch loss: 0.061\n",
      "[18] batch loss: 0.052\n",
      "[20] batch loss: 0.098\n",
      "[22] batch loss: 0.105\n",
      "[24] batch loss: 0.048\n",
      "[26] batch loss: 0.056\n",
      "[28] batch loss: 0.062\n",
      "epoch 39 loss: 2.038 -> 1.968\n",
      "\n",
      "[ 2] batch loss: 0.055\n",
      "[ 4] batch loss: 0.060\n",
      "[ 6] batch loss: 0.043\n",
      "[ 8] batch loss: 0.099\n",
      "[10] batch loss: 0.061\n",
      "[12] batch loss: 0.060\n",
      "[14] batch loss: 0.093\n",
      "[16] batch loss: 0.025\n",
      "[18] batch loss: 0.078\n",
      "[20] batch loss: 0.038\n",
      "[22] batch loss: 0.055\n",
      "[24] batch loss: 0.037\n",
      "[26] batch loss: 0.036\n",
      "[28] batch loss: 0.051\n",
      "epoch 40 loss: 1.968 -> 1.580\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 40\n",
    "finetune_epoch_start = 20\n",
    "reset_seq(cnn_da.classifier)\n",
    "reset_seq(cnn_da.label_classifier)\n",
    "\n",
    "early_stop_cnt = 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    lr = adjust_lr(optimizer, p)\n",
    "    if epoch == finetune_epoch_start:\n",
    "        for param in cnn_da.features.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr, momentum=0.9)\n",
    "    for i, data in enumerate(trainloader_source):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, _ = cnn_da(inputs)\n",
    "        loss = criterion(outputs_LC, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:    # print every 50 mini-batches\n",
    "            print('[%2d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        prev_loss = epoch_loss\n",
    "        if epoch > finetune_epoch_start:\n",
    "            early_stop_cnt += 1\n",
    "            if early_stop_cnt > 2:\n",
    "                break\n",
    "    else:\n",
    "        early_stop_cnt = 0\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.4629394645000291)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_da_accuracy(cnn_da, trainloader_source, source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.49559748427672956, 0.5306384944315976)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_da_accuracy(cnn_da, trainloader_target, source=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_da.load_pretrained_part(torch.load(\"./parameters/cnn_amazon.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in cnn_da.features.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 2.555\n",
      "[ 4] batch loss: 3.311\n",
      "[ 6] batch loss: 1.788\n",
      "[ 8] batch loss: 1.136\n",
      "[10] batch loss: 1.180\n",
      "[12] batch loss: 1.125\n",
      "[14] batch loss: 1.315\n",
      "[16] batch loss: 1.094\n",
      "[18] batch loss: 1.525\n",
      "[20] batch loss: 1.145\n",
      "[22] batch loss: 1.231\n",
      "[24] batch loss: 1.258\n",
      "[26] batch loss: 1.413\n",
      "[28] batch loss: 1.773\n",
      "epoch 1 loss: inf -> 43.699\n",
      "LC loss: inf -> 18.238\n",
      "DC loss: inf -> 25.461\n",
      "\n",
      "[ 2] batch loss: 1.577\n",
      "[ 4] batch loss: 1.207\n",
      "[ 6] batch loss: 1.312\n",
      "[ 8] batch loss: 1.251\n",
      "[10] batch loss: 1.217\n",
      "[12] batch loss: 1.098\n",
      "[14] batch loss: 1.108\n",
      "[16] batch loss: 1.194\n",
      "[18] batch loss: 1.186\n",
      "[20] batch loss: 1.127\n",
      "[22] batch loss: 1.336\n",
      "[24] batch loss: 1.252\n",
      "[26] batch loss: 1.158\n",
      "[28] batch loss: 1.309\n",
      "epoch 2 loss: 43.699 -> 34.664\n",
      "LC loss: 18.238 -> 21.686\n",
      "DC loss: 25.461 -> 12.979\n",
      "\n",
      "[ 2] batch loss: 1.082\n",
      "[ 4] batch loss: 1.038\n",
      "[ 6] batch loss: 0.913\n",
      "[ 8] batch loss: 0.765\n",
      "[10] batch loss: 0.991\n",
      "[12] batch loss: 1.047\n",
      "[14] batch loss: 0.907\n",
      "[16] batch loss: 0.994\n",
      "[18] batch loss: 1.055\n",
      "[20] batch loss: 0.909\n",
      "[22] batch loss: 0.901\n",
      "[24] batch loss: 0.880\n",
      "[26] batch loss: 0.993\n",
      "[28] batch loss: 1.135\n",
      "epoch 3 loss: 34.664 -> 27.221\n",
      "LC loss: 21.686 -> 15.291\n",
      "DC loss: 12.979 -> 11.930\n",
      "\n",
      "[ 2] batch loss: 0.971\n",
      "[ 4] batch loss: 0.812\n",
      "[ 6] batch loss: 0.697\n",
      "[ 8] batch loss: 0.749\n",
      "[10] batch loss: 0.747\n",
      "[12] batch loss: 0.942\n",
      "[14] batch loss: 0.764\n",
      "[16] batch loss: 0.750\n",
      "[18] batch loss: 0.814\n",
      "[20] batch loss: 0.882\n",
      "[22] batch loss: 0.902\n",
      "[24] batch loss: 0.876\n",
      "[26] batch loss: 1.020\n",
      "[28] batch loss: 1.156\n",
      "epoch 4 loss: 27.221 -> 24.164\n",
      "LC loss: 15.291 -> 12.717\n",
      "DC loss: 11.930 -> 11.447\n",
      "\n",
      "[ 2] batch loss: 0.790\n",
      "[ 4] batch loss: 0.779\n",
      "[ 6] batch loss: 0.866\n",
      "[ 8] batch loss: 0.633\n",
      "[10] batch loss: 0.765\n",
      "[12] batch loss: 0.866\n",
      "[14] batch loss: 0.548\n",
      "[16] batch loss: 0.700\n",
      "[18] batch loss: 0.624\n",
      "[20] batch loss: 0.760\n",
      "[22] batch loss: 0.694\n",
      "[24] batch loss: 0.789\n",
      "[26] batch loss: 0.748\n",
      "[28] batch loss: 0.805\n",
      "epoch 5 loss: 24.164 -> 20.732\n",
      "LC loss: 12.717 -> 9.824\n",
      "DC loss: 11.447 -> 10.908\n",
      "\n",
      "[ 2] batch loss: 0.608\n",
      "[ 4] batch loss: 0.794\n",
      "[ 6] batch loss: 0.641\n",
      "[ 8] batch loss: 0.608\n",
      "[10] batch loss: 0.682\n",
      "[12] batch loss: 0.781\n",
      "[14] batch loss: 0.659\n",
      "[16] batch loss: 0.738\n",
      "[18] batch loss: 0.570\n",
      "[20] batch loss: 0.514\n",
      "[22] batch loss: 0.548\n",
      "[24] batch loss: 0.635\n",
      "[26] batch loss: 0.640\n",
      "[28] batch loss: 0.696\n",
      "epoch 6 loss: 20.732 -> 18.229\n",
      "LC loss: 9.824 -> 7.396\n",
      "DC loss: 10.908 -> 10.833\n",
      "\n",
      "[ 2] batch loss: 0.542\n",
      "[ 4] batch loss: 0.609\n",
      "[ 6] batch loss: 0.534\n",
      "[ 8] batch loss: 0.574\n",
      "[10] batch loss: 0.511\n",
      "[12] batch loss: 0.523\n",
      "[14] batch loss: 0.560\n",
      "[16] batch loss: 0.502\n",
      "[18] batch loss: 0.449\n",
      "[20] batch loss: 0.589\n",
      "[22] batch loss: 0.539\n",
      "[24] batch loss: 0.453\n",
      "[26] batch loss: 0.571\n",
      "[28] batch loss: 0.505\n",
      "epoch 7 loss: 18.229 -> 14.921\n",
      "LC loss: 7.396 -> 4.903\n",
      "DC loss: 10.833 -> 10.018\n",
      "\n",
      "[ 2] batch loss: 0.482\n",
      "[ 4] batch loss: 0.439\n",
      "[ 6] batch loss: 0.431\n",
      "[ 8] batch loss: 0.474\n",
      "[10] batch loss: 0.456\n",
      "[12] batch loss: 0.497\n",
      "[14] batch loss: 0.445\n",
      "[16] batch loss: 0.489\n",
      "[18] batch loss: 0.407\n",
      "[20] batch loss: 0.436\n",
      "[22] batch loss: 0.551\n",
      "[24] batch loss: 0.395\n",
      "[26] batch loss: 0.535\n",
      "[28] batch loss: 0.603\n",
      "epoch 8 loss: 14.921 -> 13.280\n",
      "LC loss: 4.903 -> 3.621\n",
      "DC loss: 10.018 -> 9.659\n",
      "\n",
      "[ 2] batch loss: 0.414\n",
      "[ 4] batch loss: 0.514\n",
      "[ 6] batch loss: 0.440\n",
      "[ 8] batch loss: 0.496\n",
      "[10] batch loss: 0.485\n",
      "[12] batch loss: 0.503\n",
      "[14] batch loss: 0.379\n",
      "[16] batch loss: 0.422\n",
      "[18] batch loss: 0.506\n",
      "[20] batch loss: 0.425\n",
      "[22] batch loss: 0.521\n",
      "[24] batch loss: 0.460\n",
      "[26] batch loss: 0.514\n",
      "[28] batch loss: 0.494\n",
      "epoch 9 loss: 13.280 -> 13.144\n",
      "LC loss: 3.621 -> 3.554\n",
      "DC loss: 9.659 -> 9.590\n",
      "\n",
      "[ 2] batch loss: 0.451\n",
      "[ 4] batch loss: 0.496\n",
      "[ 6] batch loss: 0.495\n",
      "[ 8] batch loss: 0.436\n",
      "[10] batch loss: 0.491\n",
      "[12] batch loss: 0.417\n",
      "[14] batch loss: 0.351\n",
      "[16] batch loss: 0.435\n",
      "[18] batch loss: 0.350\n",
      "[20] batch loss: 0.441\n",
      "[22] batch loss: 0.359\n",
      "[24] batch loss: 0.431\n",
      "[26] batch loss: 0.348\n",
      "[28] batch loss: 0.487\n",
      "epoch 10 loss: 13.144 -> 11.976\n",
      "LC loss: 3.554 -> 2.992\n",
      "DC loss: 9.590 -> 8.984\n",
      "0.4327044025157233 0.426666714410362\n",
      "\n",
      "[ 2] batch loss: 0.274\n",
      "[ 4] batch loss: 0.280\n",
      "[ 6] batch loss: 0.301\n",
      "[ 8] batch loss: 0.300\n",
      "[10] batch loss: 0.308\n",
      "[12] batch loss: 0.262\n",
      "[14] batch loss: 0.268\n",
      "[16] batch loss: 0.265\n",
      "[18] batch loss: 0.261\n",
      "[20] batch loss: 0.234\n",
      "[22] batch loss: 0.226\n",
      "[24] batch loss: 0.230\n",
      "[26] batch loss: 0.244\n",
      "[28] batch loss: 0.251\n",
      "epoch 11 loss: 11.976 -> 7.407\n",
      "LC loss: 2.992 -> 0.298\n",
      "DC loss: 8.984 -> 7.109\n",
      "\n",
      "[ 2] batch loss: 0.234\n",
      "[ 4] batch loss: 0.227\n",
      "[ 6] batch loss: 0.213\n",
      "[ 8] batch loss: 0.185\n",
      "[10] batch loss: 0.230\n",
      "[12] batch loss: 0.205\n",
      "[14] batch loss: 0.190\n",
      "[16] batch loss: 0.212\n",
      "[18] batch loss: 0.205\n",
      "[20] batch loss: 0.179\n",
      "[22] batch loss: 0.195\n",
      "[24] batch loss: 0.198\n",
      "[26] batch loss: 0.197\n",
      "[28] batch loss: 0.190\n",
      "epoch 12 loss: 7.407 -> 5.723\n",
      "LC loss: 0.298 -> 0.071\n",
      "DC loss: 7.109 -> 5.652\n",
      "\n",
      "[ 2] batch loss: 0.183\n",
      "[ 4] batch loss: 0.199\n",
      "[ 6] batch loss: 0.170\n",
      "[ 8] batch loss: 0.156\n",
      "[10] batch loss: 0.205\n",
      "[12] batch loss: 0.169\n",
      "[14] batch loss: 0.167\n",
      "[16] batch loss: 0.152\n",
      "[18] batch loss: 0.158\n",
      "[20] batch loss: 0.145\n",
      "[22] batch loss: 0.170\n",
      "[24] batch loss: 0.197\n",
      "[26] batch loss: 0.172\n",
      "[28] batch loss: 0.257\n",
      "epoch 13 loss: 5.723 -> 4.999\n",
      "LC loss: 0.071 -> 0.024\n",
      "DC loss: 5.652 -> 4.975\n",
      "\n",
      "[ 2] batch loss: 0.153\n",
      "[ 4] batch loss: 0.190\n",
      "[ 6] batch loss: 0.178\n",
      "[ 8] batch loss: 0.185\n",
      "[10] batch loss: 0.183\n",
      "[12] batch loss: 0.173\n",
      "[14] batch loss: 0.220\n",
      "[16] batch loss: 0.180\n",
      "[18] batch loss: 0.169\n",
      "[20] batch loss: 0.135\n",
      "[22] batch loss: 0.146\n",
      "[24] batch loss: 0.149\n",
      "[26] batch loss: 0.153\n",
      "[28] batch loss: 0.171\n",
      "epoch 14 loss: 4.999 -> 4.775\n",
      "LC loss: 0.024 -> 0.014\n",
      "DC loss: 4.975 -> 4.761\n",
      "\n",
      "[ 2] batch loss: 0.116\n",
      "[ 4] batch loss: 0.147\n",
      "[ 6] batch loss: 0.126\n",
      "[ 8] batch loss: 0.122\n",
      "[10] batch loss: 0.150\n",
      "[12] batch loss: 0.147\n",
      "[14] batch loss: 0.172\n",
      "[16] batch loss: 0.130\n",
      "[18] batch loss: 0.147\n",
      "[20] batch loss: 0.131\n",
      "[22] batch loss: 0.165\n",
      "[24] batch loss: 0.135\n",
      "[26] batch loss: 0.164\n",
      "[28] batch loss: 0.111\n",
      "epoch 15 loss: 4.775 -> 3.928\n",
      "LC loss: 0.014 -> 0.011\n",
      "DC loss: 4.761 -> 3.917\n",
      "\n",
      "[ 2] batch loss: 0.148\n",
      "[ 4] batch loss: 0.115\n",
      "[ 6] batch loss: 0.138\n",
      "[ 8] batch loss: 0.111\n",
      "[10] batch loss: 0.119\n",
      "[12] batch loss: 0.153\n",
      "[14] batch loss: 0.093\n",
      "[16] batch loss: 0.127\n",
      "[18] batch loss: 0.136\n",
      "[20] batch loss: 0.142\n",
      "[22] batch loss: 0.149\n",
      "[24] batch loss: 0.117\n",
      "[26] batch loss: 0.124\n",
      "[28] batch loss: 0.121\n",
      "epoch 16 loss: 3.928 -> 3.587\n",
      "LC loss: 0.011 -> 0.010\n",
      "DC loss: 3.917 -> 3.577\n",
      "\n",
      "[ 2] batch loss: 0.143\n",
      "[ 4] batch loss: 0.126\n",
      "[ 6] batch loss: 0.109\n",
      "[ 8] batch loss: 0.138\n",
      "[10] batch loss: 0.132\n",
      "[12] batch loss: 0.141\n",
      "[14] batch loss: 0.145\n",
      "[16] batch loss: 0.148\n",
      "[18] batch loss: 0.105\n",
      "[20] batch loss: 0.095\n",
      "[22] batch loss: 0.117\n",
      "[24] batch loss: 0.140\n",
      "[26] batch loss: 0.115\n",
      "[28] batch loss: 0.136\n",
      "epoch 17 loss: 3.587 -> 3.582\n",
      "LC loss: 0.010 -> 0.008\n",
      "DC loss: 3.577 -> 3.573\n",
      "\n",
      "[ 2] batch loss: 0.096\n",
      "[ 4] batch loss: 0.126\n",
      "[ 6] batch loss: 0.096\n",
      "[ 8] batch loss: 0.104\n",
      "[10] batch loss: 0.099\n",
      "[12] batch loss: 0.108\n",
      "[14] batch loss: 0.112\n",
      "[16] batch loss: 0.132\n",
      "[18] batch loss: 0.113\n",
      "[20] batch loss: 0.093\n",
      "[22] batch loss: 0.114\n",
      "[24] batch loss: 0.096\n",
      "[26] batch loss: 0.099\n",
      "[28] batch loss: 0.115\n",
      "epoch 18 loss: 3.582 -> 3.010\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 3.573 -> 3.002\n",
      "\n",
      "[ 2] batch loss: 0.073\n",
      "[ 4] batch loss: 0.099\n",
      "[ 6] batch loss: 0.084\n",
      "[ 8] batch loss: 0.074\n",
      "[10] batch loss: 0.068\n",
      "[12] batch loss: 0.074\n",
      "[14] batch loss: 0.097\n",
      "[16] batch loss: 0.088\n",
      "[18] batch loss: 0.102\n",
      "[20] batch loss: 0.116\n",
      "[22] batch loss: 0.089\n",
      "[24] batch loss: 0.133\n",
      "[26] batch loss: 0.114\n",
      "[28] batch loss: 0.116\n",
      "epoch 19 loss: 3.010 -> 2.657\n",
      "LC loss: 0.008 -> 0.007\n",
      "DC loss: 3.002 -> 2.650\n",
      "\n",
      "[ 2] batch loss: 0.069\n",
      "[ 4] batch loss: 0.079\n",
      "[ 6] batch loss: 0.083\n",
      "[ 8] batch loss: 0.077\n",
      "[10] batch loss: 0.103\n",
      "[12] batch loss: 0.109\n",
      "[14] batch loss: 0.075\n",
      "[16] batch loss: 0.088\n",
      "[18] batch loss: 0.094\n",
      "[20] batch loss: 0.108\n",
      "[22] batch loss: 0.110\n",
      "[24] batch loss: 0.084\n",
      "[26] batch loss: 0.139\n",
      "[28] batch loss: 0.074\n",
      "epoch 20 loss: 2.657 -> 2.583\n",
      "LC loss: 0.007 -> 0.006\n",
      "DC loss: 2.650 -> 2.577\n",
      "0.44025157232704404 0.8814639181460975\n",
      "\n",
      "[ 2] batch loss: 0.084\n",
      "[ 4] batch loss: 0.059\n",
      "[ 6] batch loss: 0.078\n",
      "[ 8] batch loss: 0.078\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] batch loss: 0.105\n",
      "[12] batch loss: 0.109\n",
      "[14] batch loss: 0.078\n",
      "[16] batch loss: 0.104\n",
      "[18] batch loss: 0.079\n",
      "[20] batch loss: 0.082\n",
      "[22] batch loss: 0.074\n",
      "[24] batch loss: 0.105\n",
      "[26] batch loss: 0.083\n",
      "[28] batch loss: 0.105\n",
      "epoch 21 loss: 2.583 -> 2.448\n",
      "LC loss: 0.006 -> 0.006\n",
      "DC loss: 2.577 -> 2.442\n",
      "\n",
      "[ 2] batch loss: 0.059\n",
      "[ 4] batch loss: 0.084\n",
      "[ 6] batch loss: 0.068\n",
      "[ 8] batch loss: 0.085\n",
      "[10] batch loss: 0.068\n",
      "[12] batch loss: 0.093\n",
      "[14] batch loss: 0.114\n",
      "[16] batch loss: 0.067\n",
      "[18] batch loss: 0.078\n",
      "[20] batch loss: 0.059\n",
      "[22] batch loss: 0.078\n",
      "[24] batch loss: 0.063\n",
      "[26] batch loss: 0.082\n",
      "[28] batch loss: 0.082\n",
      "epoch 22 loss: 2.448 -> 2.159\n",
      "LC loss: 0.006 -> 0.005\n",
      "DC loss: 2.442 -> 2.154\n",
      "\n",
      "[ 2] batch loss: 0.053\n",
      "[ 4] batch loss: 0.072\n",
      "[ 6] batch loss: 0.060\n",
      "[ 8] batch loss: 0.079\n",
      "[10] batch loss: 0.055\n",
      "[12] batch loss: 0.076\n",
      "[14] batch loss: 0.077\n",
      "[16] batch loss: 0.075\n",
      "[18] batch loss: 0.069\n",
      "[20] batch loss: 0.068\n",
      "[22] batch loss: 0.073\n",
      "[24] batch loss: 0.095\n",
      "[26] batch loss: 0.078\n",
      "[28] batch loss: 0.092\n",
      "epoch 23 loss: 2.159 -> 2.045\n",
      "LC loss: 0.005 -> 0.005\n",
      "DC loss: 2.154 -> 2.039\n",
      "\n",
      "[ 2] batch loss: 0.067\n",
      "[ 4] batch loss: 0.062\n",
      "[ 6] batch loss: 0.097\n",
      "[ 8] batch loss: 0.077\n",
      "[10] batch loss: 0.073\n",
      "[12] batch loss: 0.076\n",
      "[14] batch loss: 0.099\n",
      "[16] batch loss: 0.090\n",
      "[18] batch loss: 0.111\n",
      "[20] batch loss: 0.066\n",
      "[22] batch loss: 0.076\n",
      "[24] batch loss: 0.079\n",
      "[26] batch loss: 0.097\n",
      "[28] batch loss: 0.089\n",
      "epoch 24 loss: 2.045 -> 2.318\n",
      "LC loss: 0.005 -> 0.005\n",
      "DC loss: 2.039 -> 2.313\n",
      "\n",
      "[ 2] batch loss: 0.068\n",
      "[ 4] batch loss: 0.075\n",
      "[ 6] batch loss: 0.093\n",
      "[ 8] batch loss: 0.095\n",
      "[10] batch loss: 0.084\n",
      "[12] batch loss: 0.058\n",
      "[14] batch loss: 0.126\n",
      "[16] batch loss: 0.082\n",
      "[18] batch loss: 0.111\n",
      "[20] batch loss: 0.081\n",
      "[22] batch loss: 0.079\n",
      "[24] batch loss: 0.069\n",
      "[26] batch loss: 0.047\n",
      "[28] batch loss: 0.114\n",
      "epoch 25 loss: 2.318 -> 2.365\n",
      "LC loss: 0.005 -> 0.005\n",
      "DC loss: 2.313 -> 2.360\n",
      "\n",
      "[ 2] batch loss: 0.049\n",
      "[ 4] batch loss: 0.083\n",
      "[ 6] batch loss: 0.055\n",
      "[ 8] batch loss: 0.064\n",
      "[10] batch loss: 0.086\n",
      "[12] batch loss: 0.053\n",
      "[14] batch loss: 0.100\n",
      "[16] batch loss: 0.052\n",
      "[18] batch loss: 0.071\n",
      "[20] batch loss: 0.051\n",
      "[22] batch loss: 0.086\n",
      "[24] batch loss: 0.046\n",
      "[26] batch loss: 0.043\n",
      "[28] batch loss: 0.105\n",
      "epoch 26 loss: 2.365 -> 1.888\n",
      "LC loss: 0.005 -> 0.004\n",
      "DC loss: 2.360 -> 1.884\n",
      "\n",
      "[ 2] batch loss: 0.048\n",
      "[ 4] batch loss: 0.052\n",
      "[ 6] batch loss: 0.058\n",
      "[ 8] batch loss: 0.092\n",
      "[10] batch loss: 0.078\n",
      "[12] batch loss: 0.063\n",
      "[14] batch loss: 0.045\n",
      "[16] batch loss: 0.039\n",
      "[18] batch loss: 0.084\n",
      "[20] batch loss: 0.039\n",
      "[22] batch loss: 0.054\n",
      "[24] batch loss: 0.044\n",
      "[26] batch loss: 0.066\n",
      "[28] batch loss: 0.034\n",
      "epoch 27 loss: 1.888 -> 1.592\n",
      "LC loss: 0.004 -> 0.004\n",
      "DC loss: 1.884 -> 1.588\n",
      "\n",
      "[ 2] batch loss: 0.066\n",
      "[ 4] batch loss: 0.059\n",
      "[ 6] batch loss: 0.059\n",
      "[ 8] batch loss: 0.039\n",
      "[10] batch loss: 0.037\n",
      "[12] batch loss: 0.041\n",
      "[14] batch loss: 0.056\n",
      "[16] batch loss: 0.084\n",
      "[18] batch loss: 0.049\n",
      "[20] batch loss: 0.027\n",
      "[22] batch loss: 0.042\n",
      "[24] batch loss: 0.035\n",
      "[26] batch loss: 0.074\n",
      "[28] batch loss: 0.045\n",
      "epoch 28 loss: 1.592 -> 1.426\n",
      "LC loss: 0.004 -> 0.004\n",
      "DC loss: 1.588 -> 1.422\n",
      "\n",
      "[ 2] batch loss: 0.042\n",
      "[ 4] batch loss: 0.043\n",
      "[ 6] batch loss: 0.051\n",
      "[ 8] batch loss: 0.047\n",
      "[10] batch loss: 0.049\n",
      "[12] batch loss: 0.067\n",
      "[14] batch loss: 0.060\n",
      "[16] batch loss: 0.044\n",
      "[18] batch loss: 0.057\n",
      "[20] batch loss: 0.057\n",
      "[22] batch loss: 0.041\n",
      "[24] batch loss: 0.053\n",
      "[26] batch loss: 0.067\n",
      "[28] batch loss: 0.081\n",
      "epoch 29 loss: 1.426 -> 1.520\n",
      "LC loss: 0.004 -> 0.004\n",
      "DC loss: 1.422 -> 1.516\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd1b06dea20>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.042\n",
      "[ 4] batch loss: 0.051\n",
      "[ 6] batch loss: 0.093\n",
      "[ 8] batch loss: 0.086\n",
      "[10] batch loss: 0.044\n",
      "[12] batch loss: 0.047\n",
      "[14] batch loss: 0.061\n",
      "[16] batch loss: 0.048\n",
      "[18] batch loss: 0.064\n",
      "[20] batch loss: 0.064\n",
      "[22] batch loss: 0.042\n",
      "[24] batch loss: 0.058\n",
      "[26] batch loss: 0.037\n",
      "[28] batch loss: 0.055\n",
      "epoch 30 loss: 1.520 -> 1.585\n",
      "LC loss: 0.004 -> 0.004\n",
      "DC loss: 1.516 -> 1.581\n",
      "0.4427672955974843 0.8865449041690466\n",
      "\n",
      "[ 2] batch loss: 0.071\n",
      "[ 4] batch loss: 0.031\n",
      "[ 6] batch loss: 0.042\n",
      "[ 8] batch loss: 0.044\n",
      "[10] batch loss: 0.060\n",
      "[12] batch loss: 0.038\n",
      "[14] batch loss: 0.036\n",
      "[16] batch loss: 0.036\n",
      "[18] batch loss: 0.050\n",
      "[20] batch loss: 0.064\n",
      "[22] batch loss: 0.066\n",
      "[24] batch loss: 0.036\n",
      "[26] batch loss: 0.038\n",
      "[28] batch loss: 0.087\n",
      "epoch 31 loss: 1.585 -> 1.399\n",
      "LC loss: 0.004 -> 0.003\n",
      "DC loss: 1.581 -> 1.395\n",
      "\n",
      "[ 2] batch loss: 0.040\n",
      "[ 4] batch loss: 0.047\n",
      "[ 6] batch loss: 0.050\n",
      "[ 8] batch loss: 0.041\n",
      "[10] batch loss: 0.043\n",
      "[12] batch loss: 0.104\n",
      "[14] batch loss: 0.040\n",
      "[16] batch loss: 0.035\n",
      "[18] batch loss: 0.054\n",
      "[20] batch loss: 0.037\n",
      "[22] batch loss: 0.051\n",
      "[24] batch loss: 0.031\n",
      "[26] batch loss: 0.045\n",
      "[28] batch loss: 0.034\n",
      "epoch 32 loss: 1.399 -> 1.305\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 1.395 -> 1.302\n",
      "\n",
      "[ 2] batch loss: 0.030\n",
      "[ 4] batch loss: 0.035\n",
      "[ 6] batch loss: 0.038\n",
      "[ 8] batch loss: 0.033\n",
      "[10] batch loss: 0.029\n",
      "[12] batch loss: 0.038\n",
      "[14] batch loss: 0.038\n",
      "[16] batch loss: 0.029\n",
      "[18] batch loss: 0.051\n",
      "[20] batch loss: 0.041\n",
      "[22] batch loss: 0.030\n",
      "[24] batch loss: 0.026\n",
      "[26] batch loss: 0.043\n",
      "[28] batch loss: 0.030\n",
      "epoch 33 loss: 1.305 -> 0.986\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 1.302 -> 0.983\n",
      "\n",
      "[ 2] batch loss: 0.046\n",
      "[ 4] batch loss: 0.038\n",
      "[ 6] batch loss: 0.029\n",
      "[ 8] batch loss: 0.028\n",
      "[10] batch loss: 0.036\n",
      "[12] batch loss: 0.061\n",
      "[14] batch loss: 0.028\n",
      "[16] batch loss: 0.024\n",
      "[18] batch loss: 0.033\n",
      "[20] batch loss: 0.061\n",
      "[22] batch loss: 0.042\n",
      "[24] batch loss: 0.053\n",
      "[26] batch loss: 0.044\n",
      "[28] batch loss: 0.051\n",
      "epoch 34 loss: 0.986 -> 1.152\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.983 -> 1.149\n",
      "\n",
      "[ 2] batch loss: 0.040\n",
      "[ 4] batch loss: 0.022\n",
      "[ 6] batch loss: 0.019\n",
      "[ 8] batch loss: 0.034\n",
      "[10] batch loss: 0.025\n",
      "[12] batch loss: 0.047\n",
      "[14] batch loss: 0.035\n",
      "[16] batch loss: 0.033\n",
      "[18] batch loss: 0.024\n",
      "[20] batch loss: 0.044\n",
      "[22] batch loss: 0.029\n",
      "[24] batch loss: 0.038\n",
      "[26] batch loss: 0.029\n",
      "[28] batch loss: 0.038\n",
      "epoch 35 loss: 1.152 -> 0.918\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 1.149 -> 0.914\n",
      "\n",
      "[ 2] batch loss: 0.020\n",
      "[ 4] batch loss: 0.028\n",
      "[ 6] batch loss: 0.021\n",
      "[ 8] batch loss: 0.029\n",
      "[10] batch loss: 0.021\n",
      "[12] batch loss: 0.040\n",
      "[14] batch loss: 0.019\n",
      "[16] batch loss: 0.027\n",
      "[18] batch loss: 0.029\n",
      "[20] batch loss: 0.053\n",
      "[22] batch loss: 0.038\n",
      "[24] batch loss: 0.089\n",
      "[26] batch loss: 0.028\n",
      "[28] batch loss: 0.051\n",
      "epoch 36 loss: 0.918 -> 0.987\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.914 -> 0.984\n",
      "\n",
      "[ 2] batch loss: 0.047\n",
      "[ 4] batch loss: 0.036\n",
      "[ 6] batch loss: 0.029\n",
      "[ 8] batch loss: 0.023\n",
      "[10] batch loss: 0.023\n",
      "[12] batch loss: 0.029\n",
      "[14] batch loss: 0.015\n",
      "[16] batch loss: 0.022\n",
      "[18] batch loss: 0.030\n",
      "[20] batch loss: 0.033\n",
      "[22] batch loss: 0.046\n",
      "[24] batch loss: 0.028\n",
      "[26] batch loss: 0.029\n",
      "[28] batch loss: 0.044\n",
      "epoch 37 loss: 0.987 -> 0.867\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.984 -> 0.864\n",
      "\n",
      "[ 2] batch loss: 0.021\n",
      "[ 4] batch loss: 0.026\n",
      "[ 6] batch loss: 0.028\n",
      "[ 8] batch loss: 0.019\n",
      "[10] batch loss: 0.026\n",
      "[12] batch loss: 0.030\n",
      "[14] batch loss: 0.028\n",
      "[16] batch loss: 0.031\n",
      "[18] batch loss: 0.016\n",
      "[20] batch loss: 0.027\n",
      "[22] batch loss: 0.026\n",
      "[24] batch loss: 0.026\n",
      "[26] batch loss: 0.033\n",
      "[28] batch loss: 0.033\n",
      "epoch 38 loss: 0.867 -> 0.740\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.864 -> 0.737\n",
      "\n",
      "[ 2] batch loss: 0.017\n",
      "[ 4] batch loss: 0.016\n",
      "[ 6] batch loss: 0.019\n",
      "[ 8] batch loss: 0.024\n",
      "[10] batch loss: 0.015\n",
      "[12] batch loss: 0.021\n",
      "[14] batch loss: 0.026\n",
      "[16] batch loss: 0.014\n",
      "[18] batch loss: 0.027\n",
      "[20] batch loss: 0.029\n",
      "[22] batch loss: 0.025\n",
      "[24] batch loss: 0.018\n",
      "[26] batch loss: 0.036\n",
      "[28] batch loss: 0.031\n",
      "epoch 39 loss: 0.740 -> 0.635\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.737 -> 0.632\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd1b06de9b0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 155, in recvfds\n",
      "    raise EOFError\n",
      "EOFError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.020\n",
      "[ 4] batch loss: 0.019\n",
      "[ 6] batch loss: 0.015\n",
      "[ 8] batch loss: 0.021\n",
      "[10] batch loss: 0.014\n",
      "[12] batch loss: 0.017\n",
      "[14] batch loss: 0.019\n",
      "[16] batch loss: 0.033\n",
      "[18] batch loss: 0.021\n",
      "[20] batch loss: 0.020\n",
      "[22] batch loss: 0.019\n",
      "[24] batch loss: 0.015\n",
      "[26] batch loss: 0.021\n",
      "[28] batch loss: 0.023\n",
      "epoch 40 loss: 0.635 -> 0.552\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.632 -> 0.549\n",
      "0.4427672955974843 0.9664286007671237\n",
      "\n",
      "[ 2] batch loss: 0.017\n",
      "[ 4] batch loss: 0.014\n",
      "[ 6] batch loss: 0.019\n",
      "[ 8] batch loss: 0.019\n",
      "[10] batch loss: 0.043\n",
      "[12] batch loss: 0.023\n",
      "[14] batch loss: 0.025\n",
      "[16] batch loss: 0.017\n",
      "[18] batch loss: 0.014\n",
      "[20] batch loss: 0.033\n",
      "[22] batch loss: 0.015\n",
      "[24] batch loss: 0.026\n",
      "[26] batch loss: 0.029\n",
      "[28] batch loss: 0.018\n",
      "epoch 41 loss: 0.552 -> 0.625\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.549 -> 0.622\n",
      "\n",
      "[ 2] batch loss: 0.012\n",
      "[ 4] batch loss: 0.033\n",
      "[ 6] batch loss: 0.045\n",
      "[ 8] batch loss: 0.015\n",
      "[10] batch loss: 0.018\n",
      "[12] batch loss: 0.021\n",
      "[14] batch loss: 0.036\n",
      "[16] batch loss: 0.015\n",
      "[18] batch loss: 0.015\n",
      "[20] batch loss: 0.033\n",
      "[22] batch loss: 0.014\n",
      "[24] batch loss: 0.020\n",
      "[26] batch loss: 0.017\n",
      "[28] batch loss: 0.021\n",
      "epoch 42 loss: 0.625 -> 0.632\n",
      "LC loss: 0.003 -> 0.003\n",
      "DC loss: 0.622 -> 0.629\n",
      "\n",
      "[ 2] batch loss: 0.020\n",
      "[ 4] batch loss: 0.021\n",
      "[ 6] batch loss: 0.016\n",
      "[ 8] batch loss: 0.017\n",
      "[10] batch loss: 0.019\n",
      "[12] batch loss: 0.013\n",
      "[14] batch loss: 0.018\n",
      "[16] batch loss: 0.019\n",
      "[18] batch loss: 0.013\n",
      "[20] batch loss: 0.040\n",
      "[22] batch loss: 0.018\n",
      "[24] batch loss: 0.022\n",
      "[26] batch loss: 0.020\n",
      "[28] batch loss: 0.018\n",
      "epoch 43 loss: 0.632 -> 0.544\n",
      "LC loss: 0.003 -> 0.002\n",
      "DC loss: 0.629 -> 0.542\n",
      "\n",
      "[ 2] batch loss: 0.033\n",
      "[ 4] batch loss: 0.014\n",
      "[ 6] batch loss: 0.013\n",
      "[ 8] batch loss: 0.011\n",
      "[10] batch loss: 0.018\n",
      "[12] batch loss: 0.023\n",
      "[14] batch loss: 0.018\n",
      "[16] batch loss: 0.011\n",
      "[18] batch loss: 0.022\n",
      "[20] batch loss: 0.019\n",
      "[22] batch loss: 0.020\n",
      "[24] batch loss: 0.013\n",
      "[26] batch loss: 0.015\n",
      "[28] batch loss: 0.041\n",
      "epoch 44 loss: 0.544 -> 0.542\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.542 -> 0.539\n",
      "\n",
      "[ 2] batch loss: 0.021\n",
      "[ 4] batch loss: 0.022\n",
      "[ 6] batch loss: 0.034\n",
      "[ 8] batch loss: 0.012\n",
      "[10] batch loss: 0.020\n",
      "[12] batch loss: 0.012\n",
      "[14] batch loss: 0.025\n",
      "[16] batch loss: 0.029\n",
      "[18] batch loss: 0.019\n",
      "[20] batch loss: 0.017\n",
      "[22] batch loss: 0.030\n",
      "[24] batch loss: 0.015\n",
      "[26] batch loss: 0.011\n",
      "[28] batch loss: 0.023\n",
      "epoch 45 loss: 0.542 -> 0.581\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.539 -> 0.579\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd1b06e75c0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.012\n",
      "[ 4] batch loss: 0.017\n",
      "[ 6] batch loss: 0.038\n",
      "[ 8] batch loss: 0.013\n",
      "[10] batch loss: 0.015\n",
      "[12] batch loss: 0.019\n",
      "[14] batch loss: 0.018\n",
      "[16] batch loss: 0.015\n",
      "[18] batch loss: 0.016\n",
      "[20] batch loss: 0.013\n",
      "[22] batch loss: 0.035\n",
      "[24] batch loss: 0.048\n",
      "[26] batch loss: 0.018\n",
      "[28] batch loss: 0.018\n",
      "epoch 46 loss: 0.581 -> 0.589\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.579 -> 0.587\n",
      "\n",
      "[ 2] batch loss: 0.021\n",
      "[ 4] batch loss: 0.015\n",
      "[ 6] batch loss: 0.029\n",
      "[ 8] batch loss: 0.010\n",
      "[10] batch loss: 0.013\n",
      "[12] batch loss: 0.016\n",
      "[14] batch loss: 0.028\n",
      "[16] batch loss: 0.018\n",
      "[18] batch loss: 0.017\n",
      "[20] batch loss: 0.027\n",
      "[22] batch loss: 0.028\n",
      "[24] batch loss: 0.024\n",
      "[26] batch loss: 0.017\n",
      "[28] batch loss: 0.009\n",
      "epoch 47 loss: 0.589 -> 0.542\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.587 -> 0.540\n",
      "\n",
      "[ 2] batch loss: 0.010\n",
      "[ 4] batch loss: 0.026\n",
      "[ 6] batch loss: 0.019\n",
      "[ 8] batch loss: 0.019\n",
      "[10] batch loss: 0.011\n",
      "[12] batch loss: 0.017\n",
      "[14] batch loss: 0.018\n",
      "[16] batch loss: 0.024\n",
      "[18] batch loss: 0.010\n",
      "[20] batch loss: 0.011\n",
      "[22] batch loss: 0.014\n",
      "[24] batch loss: 0.011\n",
      "[26] batch loss: 0.015\n",
      "[28] batch loss: 0.027\n",
      "epoch 48 loss: 0.542 -> 0.466\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.540 -> 0.464\n",
      "\n",
      "[ 2] batch loss: 0.026\n",
      "[ 4] batch loss: 0.024\n",
      "[ 6] batch loss: 0.034\n",
      "[ 8] batch loss: 0.010\n",
      "[10] batch loss: 0.015\n",
      "[12] batch loss: 0.017\n",
      "[14] batch loss: 0.031\n",
      "[16] batch loss: 0.019\n",
      "[18] batch loss: 0.016\n",
      "[20] batch loss: 0.011\n",
      "[22] batch loss: 0.009\n",
      "[24] batch loss: 0.019\n",
      "[26] batch loss: 0.012\n",
      "[28] batch loss: 0.017\n",
      "epoch 49 loss: 0.466 -> 0.523\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.464 -> 0.521\n",
      "\n",
      "[ 2] batch loss: 0.026\n",
      "[ 4] batch loss: 0.011\n",
      "[ 6] batch loss: 0.010\n",
      "[ 8] batch loss: 0.009\n",
      "[10] batch loss: 0.012\n",
      "[12] batch loss: 0.018\n",
      "[14] batch loss: 0.014\n",
      "[16] batch loss: 0.022\n",
      "[18] batch loss: 0.011\n",
      "[20] batch loss: 0.017\n",
      "[22] batch loss: 0.009\n",
      "[24] batch loss: 0.012\n",
      "[26] batch loss: 0.011\n",
      "[28] batch loss: 0.010\n",
      "epoch 50 loss: 0.523 -> 0.383\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.521 -> 0.381\n",
      "0.44528301886792454 0.9555573601392829\n",
      "\n",
      "[ 2] batch loss: 0.007\n",
      "[ 4] batch loss: 0.015\n",
      "[ 6] batch loss: 0.010\n",
      "[ 8] batch loss: 0.016\n",
      "[10] batch loss: 0.019\n",
      "[12] batch loss: 0.008\n",
      "[14] batch loss: 0.009\n",
      "[16] batch loss: 0.016\n",
      "[18] batch loss: 0.009\n",
      "[20] batch loss: 0.019\n",
      "[22] batch loss: 0.014\n",
      "[24] batch loss: 0.011\n",
      "[26] batch loss: 0.011\n",
      "[28] batch loss: 0.021\n",
      "epoch 51 loss: 0.383 -> 0.368\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.381 -> 0.366\n",
      "\n",
      "[ 2] batch loss: 0.014\n",
      "[ 4] batch loss: 0.017\n",
      "[ 6] batch loss: 0.010\n",
      "[ 8] batch loss: 0.012\n",
      "[10] batch loss: 0.014\n",
      "[12] batch loss: 0.009\n",
      "[14] batch loss: 0.011\n",
      "[16] batch loss: 0.016\n",
      "[18] batch loss: 0.009\n",
      "[20] batch loss: 0.010\n",
      "[22] batch loss: 0.013\n",
      "[24] batch loss: 0.009\n",
      "[26] batch loss: 0.012\n",
      "[28] batch loss: 0.016\n",
      "epoch 52 loss: 0.368 -> 0.344\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.366 -> 0.341\n",
      "\n",
      "[ 2] batch loss: 0.012\n",
      "[ 4] batch loss: 0.008\n",
      "[ 6] batch loss: 0.018\n",
      "[ 8] batch loss: 0.012\n",
      "[10] batch loss: 0.013\n",
      "[12] batch loss: 0.010\n",
      "[14] batch loss: 0.011\n",
      "[16] batch loss: 0.007\n",
      "[18] batch loss: 0.011\n",
      "[20] batch loss: 0.012\n",
      "[22] batch loss: 0.007\n",
      "[24] batch loss: 0.010\n",
      "[26] batch loss: 0.008\n",
      "[28] batch loss: 0.011\n",
      "epoch 53 loss: 0.344 -> 0.302\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.341 -> 0.300\n",
      "\n",
      "[ 2] batch loss: 0.008\n",
      "[ 4] batch loss: 0.013\n",
      "[ 6] batch loss: 0.009\n",
      "[ 8] batch loss: 0.007\n",
      "[10] batch loss: 0.007\n",
      "[12] batch loss: 0.010\n",
      "[14] batch loss: 0.007\n",
      "[16] batch loss: 0.011\n",
      "[18] batch loss: 0.010\n",
      "[20] batch loss: 0.010\n",
      "[22] batch loss: 0.010\n",
      "[24] batch loss: 0.016\n",
      "[26] batch loss: 0.008\n",
      "[28] batch loss: 0.008\n",
      "epoch 54 loss: 0.302 -> 0.270\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.300 -> 0.268\n",
      "\n",
      "[ 2] batch loss: 0.014\n",
      "[ 4] batch loss: 0.010\n",
      "[ 6] batch loss: 0.010\n",
      "[ 8] batch loss: 0.008\n",
      "[10] batch loss: 0.006\n",
      "[12] batch loss: 0.007\n",
      "[14] batch loss: 0.008\n",
      "[16] batch loss: 0.007\n",
      "[18] batch loss: 0.013\n",
      "[20] batch loss: 0.009\n",
      "[22] batch loss: 0.008\n",
      "[24] batch loss: 0.008\n",
      "[26] batch loss: 0.009\n",
      "[28] batch loss: 0.009\n",
      "epoch 55 loss: 0.270 -> 0.252\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.268 -> 0.250\n",
      "\n",
      "[ 2] batch loss: 0.007\n",
      "[ 4] batch loss: 0.012\n",
      "[ 6] batch loss: 0.010\n",
      "[ 8] batch loss: 0.010\n",
      "[10] batch loss: 0.009\n",
      "[12] batch loss: 0.017\n",
      "[14] batch loss: 0.011\n",
      "[16] batch loss: 0.018\n",
      "[18] batch loss: 0.010\n",
      "[20] batch loss: 0.020\n",
      "[22] batch loss: 0.005\n",
      "[24] batch loss: 0.011\n",
      "[26] batch loss: 0.019\n",
      "[28] batch loss: 0.015\n",
      "epoch 56 loss: 0.252 -> 0.348\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.250 -> 0.346\n",
      "\n",
      "[ 2] batch loss: 0.008\n",
      "[ 4] batch loss: 0.016\n",
      "[ 6] batch loss: 0.008\n",
      "[ 8] batch loss: 0.010\n",
      "[10] batch loss: 0.027\n",
      "[12] batch loss: 0.010\n",
      "[14] batch loss: 0.013\n",
      "[16] batch loss: 0.008\n",
      "[18] batch loss: 0.012\n",
      "[20] batch loss: 0.009\n",
      "[22] batch loss: 0.014\n",
      "[24] batch loss: 0.011\n",
      "[26] batch loss: 0.012\n",
      "[28] batch loss: 0.012\n",
      "epoch 57 loss: 0.348 -> 0.337\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.346 -> 0.335\n",
      "\n",
      "[ 2] batch loss: 0.008\n",
      "[ 4] batch loss: 0.008\n",
      "[ 6] batch loss: 0.011\n",
      "[ 8] batch loss: 0.008\n",
      "[10] batch loss: 0.007\n",
      "[12] batch loss: 0.018\n",
      "[14] batch loss: 0.015\n",
      "[16] batch loss: 0.012\n",
      "[18] batch loss: 0.016\n",
      "[20] batch loss: 0.019\n",
      "[22] batch loss: 0.008\n",
      "[24] batch loss: 0.009\n",
      "[26] batch loss: 0.022\n",
      "[28] batch loss: 0.011\n",
      "epoch 58 loss: 0.337 -> 0.342\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.335 -> 0.340\n",
      "\n",
      "[ 2] batch loss: 0.015\n",
      "[ 4] batch loss: 0.015\n",
      "[ 6] batch loss: 0.014\n",
      "[ 8] batch loss: 0.017\n",
      "[10] batch loss: 0.020\n",
      "[12] batch loss: 0.007\n",
      "[14] batch loss: 0.012\n",
      "[16] batch loss: 0.024\n",
      "[18] batch loss: 0.018\n",
      "[20] batch loss: 0.013\n",
      "[22] batch loss: 0.010\n",
      "[24] batch loss: 0.014\n",
      "[26] batch loss: 0.004\n",
      "[28] batch loss: 0.036\n",
      "epoch 59 loss: 0.342 -> 0.438\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.340 -> 0.436\n",
      "\n",
      "[ 2] batch loss: 0.008\n",
      "[ 4] batch loss: 0.010\n",
      "[ 6] batch loss: 0.008\n",
      "[ 8] batch loss: 0.021\n",
      "[10] batch loss: 0.011\n",
      "[12] batch loss: 0.017\n",
      "[14] batch loss: 0.017\n",
      "[16] batch loss: 0.030\n",
      "[18] batch loss: 0.007\n",
      "[20] batch loss: 0.018\n",
      "[22] batch loss: 0.011\n",
      "[24] batch loss: 0.007\n",
      "[26] batch loss: 0.008\n",
      "[28] batch loss: 0.009\n",
      "epoch 60 loss: 0.438 -> 0.362\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.436 -> 0.360\n",
      "0.4440251572327044 0.9688603982985394\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd1b06e7a58>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 155, in recvfds\n",
      "    raise EOFError\n",
      "EOFError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.016\n",
      "[ 4] batch loss: 0.011\n",
      "[ 6] batch loss: 0.007\n",
      "[ 8] batch loss: 0.007\n",
      "[10] batch loss: 0.009\n",
      "[12] batch loss: 0.006\n",
      "[14] batch loss: 0.007\n",
      "[16] batch loss: 0.014\n",
      "[18] batch loss: 0.008\n",
      "[20] batch loss: 0.017\n",
      "[22] batch loss: 0.006\n",
      "[24] batch loss: 0.008\n",
      "[26] batch loss: 0.008\n",
      "[28] batch loss: 0.010\n",
      "epoch 61 loss: 0.362 -> 0.273\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.360 -> 0.272\n",
      "\n",
      "[ 2] batch loss: 0.006\n",
      "[ 4] batch loss: 0.007\n",
      "[ 6] batch loss: 0.010\n",
      "[ 8] batch loss: 0.009\n",
      "[10] batch loss: 0.007\n",
      "[12] batch loss: 0.012\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.007\n",
      "[18] batch loss: 0.008\n",
      "[20] batch loss: 0.005\n",
      "[22] batch loss: 0.009\n",
      "[24] batch loss: 0.005\n",
      "[26] batch loss: 0.008\n",
      "[28] batch loss: 0.015\n",
      "epoch 62 loss: 0.273 -> 0.224\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.272 -> 0.222\n",
      "\n",
      "[ 2] batch loss: 0.010\n",
      "[ 4] batch loss: 0.007\n",
      "[ 6] batch loss: 0.005\n",
      "[ 8] batch loss: 0.009\n",
      "[10] batch loss: 0.007\n",
      "[12] batch loss: 0.005\n",
      "[14] batch loss: 0.007\n",
      "[16] batch loss: 0.008\n",
      "[18] batch loss: 0.006\n",
      "[20] batch loss: 0.011\n",
      "[22] batch loss: 0.006\n",
      "[24] batch loss: 0.008\n",
      "[26] batch loss: 0.006\n",
      "[28] batch loss: 0.011\n",
      "epoch 63 loss: 0.224 -> 0.215\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.222 -> 0.214\n",
      "\n",
      "[ 2] batch loss: 0.007\n",
      "[ 4] batch loss: 0.010\n",
      "[ 6] batch loss: 0.005\n",
      "[ 8] batch loss: 0.006\n",
      "[10] batch loss: 0.006\n",
      "[12] batch loss: 0.006\n",
      "[14] batch loss: 0.006\n",
      "[16] batch loss: 0.006\n",
      "[18] batch loss: 0.005\n",
      "[20] batch loss: 0.005\n",
      "[22] batch loss: 0.006\n",
      "[24] batch loss: 0.007\n",
      "[26] batch loss: 0.007\n",
      "[28] batch loss: 0.006\n",
      "epoch 64 loss: 0.215 -> 0.180\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.214 -> 0.178\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd1b06e7630>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 153, in recvfds\n",
      "    msg, ancdata, flags, addr = sock.recvmsg(1, socket.CMSG_LEN(bytes_size))\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.004\n",
      "[ 4] batch loss: 0.006\n",
      "[ 6] batch loss: 0.007\n",
      "[ 8] batch loss: 0.007\n",
      "[10] batch loss: 0.005\n",
      "[12] batch loss: 0.005\n",
      "[14] batch loss: 0.007\n",
      "[16] batch loss: 0.006\n",
      "[18] batch loss: 0.005\n",
      "[20] batch loss: 0.005\n",
      "[22] batch loss: 0.005\n",
      "[24] batch loss: 0.006\n",
      "[26] batch loss: 0.008\n",
      "[28] batch loss: 0.006\n",
      "epoch 65 loss: 0.180 -> 0.164\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.178 -> 0.162\n",
      "\n",
      "[ 2] batch loss: 0.007\n",
      "[ 4] batch loss: 0.005\n",
      "[ 6] batch loss: 0.005\n",
      "[ 8] batch loss: 0.008\n",
      "[10] batch loss: 0.006\n",
      "[12] batch loss: 0.005\n",
      "[14] batch loss: 0.006\n",
      "[16] batch loss: 0.005\n",
      "[18] batch loss: 0.003\n",
      "[20] batch loss: 0.009\n",
      "[22] batch loss: 0.006\n",
      "[24] batch loss: 0.007\n",
      "[26] batch loss: 0.004\n",
      "[28] batch loss: 0.007\n",
      "epoch 66 loss: 0.164 -> 0.168\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.162 -> 0.166\n",
      "\n",
      "[ 2] batch loss: 0.004\n",
      "[ 4] batch loss: 0.009\n",
      "[ 6] batch loss: 0.007\n",
      "[ 8] batch loss: 0.006\n",
      "[10] batch loss: 0.005\n",
      "[12] batch loss: 0.005\n",
      "[14] batch loss: 0.010\n",
      "[16] batch loss: 0.002\n",
      "[18] batch loss: 0.007\n",
      "[20] batch loss: 0.005\n",
      "[22] batch loss: 0.005\n",
      "[24] batch loss: 0.006\n",
      "[26] batch loss: 0.005\n",
      "[28] batch loss: 0.006\n",
      "epoch 67 loss: 0.168 -> 0.169\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.166 -> 0.167\n",
      "\n",
      "[ 2] batch loss: 0.005\n",
      "[ 4] batch loss: 0.005\n",
      "[ 6] batch loss: 0.005\n",
      "[ 8] batch loss: 0.005\n",
      "[10] batch loss: 0.007\n",
      "[12] batch loss: 0.006\n",
      "[14] batch loss: 0.005\n",
      "[16] batch loss: 0.007\n",
      "[18] batch loss: 0.004\n",
      "[20] batch loss: 0.008\n",
      "[22] batch loss: 0.008\n",
      "[24] batch loss: 0.006\n",
      "[26] batch loss: 0.007\n",
      "[28] batch loss: 0.005\n",
      "epoch 68 loss: 0.169 -> 0.164\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.167 -> 0.162\n",
      "\n",
      "[ 2] batch loss: 0.005\n",
      "[ 4] batch loss: 0.006\n",
      "[ 6] batch loss: 0.007\n",
      "[ 8] batch loss: 0.005\n",
      "[10] batch loss: 0.006\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.005\n",
      "[16] batch loss: 0.008\n",
      "[18] batch loss: 0.007\n",
      "[20] batch loss: 0.003\n",
      "[22] batch loss: 0.005\n",
      "[24] batch loss: 0.005\n",
      "[26] batch loss: 0.004\n",
      "[28] batch loss: 0.011\n",
      "epoch 69 loss: 0.164 -> 0.162\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.162 -> 0.160\n",
      "\n",
      "[ 2] batch loss: 0.006\n",
      "[ 4] batch loss: 0.005\n",
      "[ 6] batch loss: 0.008\n",
      "[ 8] batch loss: 0.005\n",
      "[10] batch loss: 0.005\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.005\n",
      "[18] batch loss: 0.006\n",
      "[20] batch loss: 0.005\n",
      "[22] batch loss: 0.007\n",
      "[24] batch loss: 0.006\n",
      "[26] batch loss: 0.006\n",
      "[28] batch loss: 0.007\n",
      "epoch 70 loss: 0.162 -> 0.154\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.160 -> 0.153\n",
      "0.4440251572327044 0.9882631253896269\n",
      "\n",
      "[ 2] batch loss: 0.005\n",
      "[ 4] batch loss: 0.005\n",
      "[ 6] batch loss: 0.006\n",
      "[ 8] batch loss: 0.004\n",
      "[10] batch loss: 0.004\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.008\n",
      "[16] batch loss: 0.006\n",
      "[18] batch loss: 0.011\n",
      "[20] batch loss: 0.010\n",
      "[22] batch loss: 0.006\n",
      "[24] batch loss: 0.005\n",
      "[26] batch loss: 0.007\n",
      "[28] batch loss: 0.004\n",
      "epoch 71 loss: 0.154 -> 0.169\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.153 -> 0.168\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd21cdf30b8>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 57, in detach\n",
      "    with _resource_sharer.get_connection(self._id) as conn:\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 87, in get_connection\n",
      "    c = Client(address, authkey=process.current_process().authkey)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 493, in Client\n",
      "    answer_challenge(c, authkey)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 737, in answer_challenge\n",
      "    response = connection.recv_bytes(256)        # reject large message\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "ConnectionResetError: [Errno 104] Connection reset by peer\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.008\n",
      "[ 4] batch loss: 0.005\n",
      "[ 6] batch loss: 0.003\n",
      "[ 8] batch loss: 0.004\n",
      "[10] batch loss: 0.006\n",
      "[12] batch loss: 0.005\n",
      "[14] batch loss: 0.007\n",
      "[16] batch loss: 0.006\n",
      "[18] batch loss: 0.006\n",
      "[20] batch loss: 0.003\n",
      "[22] batch loss: 0.005\n",
      "[24] batch loss: 0.006\n",
      "[26] batch loss: 0.005\n",
      "[28] batch loss: 0.007\n",
      "epoch 72 loss: 0.169 -> 0.150\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.168 -> 0.149\n",
      "\n",
      "[ 2] batch loss: 0.005\n",
      "[ 4] batch loss: 0.007\n",
      "[ 6] batch loss: 0.005\n",
      "[ 8] batch loss: 0.005\n",
      "[10] batch loss: 0.004\n",
      "[12] batch loss: 0.005\n",
      "[14] batch loss: 0.005\n",
      "[16] batch loss: 0.007\n",
      "[18] batch loss: 0.003\n",
      "[20] batch loss: 0.004\n",
      "[22] batch loss: 0.006\n",
      "[24] batch loss: 0.008\n",
      "[26] batch loss: 0.006\n",
      "[28] batch loss: 0.005\n",
      "epoch 73 loss: 0.150 -> 0.151\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.149 -> 0.150\n",
      "\n",
      "[ 2] batch loss: 0.005\n",
      "[ 4] batch loss: 0.003\n",
      "[ 6] batch loss: 0.007\n",
      "[ 8] batch loss: 0.003\n",
      "[10] batch loss: 0.004\n",
      "[12] batch loss: 0.005\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.006\n",
      "[18] batch loss: 0.006\n",
      "[20] batch loss: 0.004\n",
      "[22] batch loss: 0.006\n",
      "[24] batch loss: 0.005\n",
      "[26] batch loss: 0.003\n",
      "[28] batch loss: 0.005\n",
      "epoch 74 loss: 0.151 -> 0.133\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.150 -> 0.131\n",
      "\n",
      "[ 2] batch loss: 0.005\n",
      "[ 4] batch loss: 0.004\n",
      "[ 6] batch loss: 0.003\n",
      "[ 8] batch loss: 0.003\n",
      "[10] batch loss: 0.004\n",
      "[12] batch loss: 0.007\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.005\n",
      "[18] batch loss: 0.003\n",
      "[20] batch loss: 0.004\n",
      "[22] batch loss: 0.007\n",
      "[24] batch loss: 0.007\n",
      "[26] batch loss: 0.003\n",
      "[28] batch loss: 0.006\n",
      "epoch 75 loss: 0.133 -> 0.131\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.131 -> 0.130\n",
      "\n",
      "[ 2] batch loss: 0.006\n",
      "[ 4] batch loss: 0.003\n",
      "[ 6] batch loss: 0.005\n",
      "[ 8] batch loss: 0.005\n",
      "[10] batch loss: 0.005\n",
      "[12] batch loss: 0.003\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.005\n",
      "[18] batch loss: 0.004\n",
      "[20] batch loss: 0.005\n",
      "[22] batch loss: 0.004\n",
      "[24] batch loss: 0.008\n",
      "[26] batch loss: 0.004\n",
      "[28] batch loss: 0.003\n",
      "epoch 76 loss: 0.131 -> 0.128\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.130 -> 0.126\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7fd1b06e7320>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 349, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 328, in _shutdown_workers\n",
      "    self.worker_result_queue.get()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/queues.py\", line 337, in get\n",
      "    return _ForkingPickler.loads(res)\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/torch/multiprocessing/reductions.py\", line 70, in rebuild_storage_fd\n",
      "    fd = df.detach()\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/resource_sharer.py\", line 58, in detach\n",
      "    return reduction.recv_handle(conn)\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 182, in recv_handle\n",
      "    return recvfds(s, 1)[0]\n",
      "  File \"/opt/conda/lib/python3.6/multiprocessing/reduction.py\", line 155, in recvfds\n",
      "    raise EOFError\n",
      "EOFError: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.004\n",
      "[ 4] batch loss: 0.006\n",
      "[ 6] batch loss: 0.004\n",
      "[ 8] batch loss: 0.003\n",
      "[10] batch loss: 0.005\n",
      "[12] batch loss: 0.003\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.006\n",
      "[18] batch loss: 0.003\n",
      "[20] batch loss: 0.007\n",
      "[22] batch loss: 0.004\n",
      "[24] batch loss: 0.004\n",
      "[26] batch loss: 0.005\n",
      "[28] batch loss: 0.003\n",
      "epoch 77 loss: 0.128 -> 0.122\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.126 -> 0.120\n",
      "\n",
      "[ 2] batch loss: 0.003\n",
      "[ 4] batch loss: 0.004\n",
      "[ 6] batch loss: 0.004\n",
      "[ 8] batch loss: 0.004\n",
      "[10] batch loss: 0.005\n",
      "[12] batch loss: 0.003\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.005\n",
      "[18] batch loss: 0.005\n",
      "[20] batch loss: 0.003\n",
      "[22] batch loss: 0.004\n",
      "[24] batch loss: 0.008\n",
      "[26] batch loss: 0.004\n",
      "[28] batch loss: 0.004\n",
      "epoch 78 loss: 0.122 -> 0.115\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.120 -> 0.113\n",
      "\n",
      "[ 2] batch loss: 0.005\n",
      "[ 4] batch loss: 0.004\n",
      "[ 6] batch loss: 0.004\n",
      "[ 8] batch loss: 0.004\n",
      "[10] batch loss: 0.004\n",
      "[12] batch loss: 0.002\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.002\n",
      "[18] batch loss: 0.004\n",
      "[20] batch loss: 0.004\n",
      "[22] batch loss: 0.003\n",
      "[24] batch loss: 0.004\n",
      "[26] batch loss: 0.005\n",
      "[28] batch loss: 0.005\n",
      "epoch 79 loss: 0.115 -> 0.108\n",
      "LC loss: 0.002 -> 0.002\n",
      "DC loss: 0.113 -> 0.106\n",
      "\n",
      "[ 2] batch loss: 0.003\n",
      "[ 4] batch loss: 0.004\n",
      "[ 6] batch loss: 0.005\n",
      "[ 8] batch loss: 0.003\n",
      "[10] batch loss: 0.003\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.003\n",
      "[16] batch loss: 0.003\n",
      "[18] batch loss: 0.004\n",
      "[20] batch loss: 0.004\n",
      "[22] batch loss: 0.004\n",
      "[24] batch loss: 0.004\n",
      "[26] batch loss: 0.003\n",
      "[28] batch loss: 0.005\n",
      "epoch 80 loss: 0.108 -> 0.104\n",
      "LC loss: 0.002 -> 0.001\n",
      "DC loss: 0.106 -> 0.103\n",
      "0.44654088050314467 0.9887060537278277\n",
      "\n",
      "[ 2] batch loss: 0.003\n",
      "[ 4] batch loss: 0.008\n",
      "[ 6] batch loss: 0.003\n",
      "[ 8] batch loss: 0.003\n",
      "[10] batch loss: 0.003\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.007\n",
      "[16] batch loss: 0.004\n",
      "[18] batch loss: 0.004\n",
      "[20] batch loss: 0.002\n",
      "[22] batch loss: 0.004\n",
      "[24] batch loss: 0.003\n",
      "[26] batch loss: 0.003\n",
      "[28] batch loss: 0.005\n",
      "epoch 81 loss: 0.104 -> 0.111\n",
      "LC loss: 0.001 -> 0.001\n",
      "DC loss: 0.103 -> 0.110\n",
      "\n",
      "[ 2] batch loss: 0.003\n",
      "[ 4] batch loss: 0.003\n",
      "[ 6] batch loss: 0.004\n",
      "[ 8] batch loss: 0.004\n",
      "[10] batch loss: 0.003\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.005\n",
      "[16] batch loss: 0.003\n",
      "[18] batch loss: 0.003\n",
      "[20] batch loss: 0.003\n",
      "[22] batch loss: 0.003\n",
      "[24] batch loss: 0.003\n",
      "[26] batch loss: 0.006\n",
      "[28] batch loss: 0.005\n",
      "epoch 82 loss: 0.111 -> 0.104\n",
      "LC loss: 0.001 -> 0.001\n",
      "DC loss: 0.110 -> 0.103\n",
      "\n",
      "[ 2] batch loss: 0.003\n",
      "[ 4] batch loss: 0.004\n",
      "[ 6] batch loss: 0.003\n",
      "[ 8] batch loss: 0.005\n",
      "[10] batch loss: 0.003\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.005\n",
      "[16] batch loss: 0.004\n",
      "[18] batch loss: 0.003\n",
      "[20] batch loss: 0.004\n",
      "[22] batch loss: 0.003\n",
      "[24] batch loss: 0.003\n",
      "[26] batch loss: 0.002\n",
      "[28] batch loss: 0.003\n",
      "epoch 83 loss: 0.104 -> 0.097\n",
      "LC loss: 0.001 -> 0.001\n",
      "DC loss: 0.103 -> 0.095\n",
      "\n",
      "[ 2] batch loss: 0.004\n",
      "[ 4] batch loss: 0.003\n",
      "[ 6] batch loss: 0.004\n",
      "[ 8] batch loss: 0.003\n",
      "[10] batch loss: 0.003\n",
      "[12] batch loss: 0.002\n",
      "[14] batch loss: 0.003\n",
      "[16] batch loss: 0.003\n",
      "[18] batch loss: 0.005\n",
      "[20] batch loss: 0.004\n",
      "[22] batch loss: 0.003\n",
      "[24] batch loss: 0.004\n",
      "[26] batch loss: 0.004\n",
      "[28] batch loss: 0.003\n",
      "epoch 84 loss: 0.097 -> 0.096\n",
      "LC loss: 0.001 -> 0.001\n",
      "DC loss: 0.095 -> 0.094\n",
      "\n",
      "[ 2] batch loss: 0.002\n",
      "[ 4] batch loss: 0.003\n",
      "[ 6] batch loss: 0.003\n",
      "[ 8] batch loss: 0.003\n",
      "[10] batch loss: 0.003\n",
      "[12] batch loss: 0.004\n",
      "[14] batch loss: 0.004\n",
      "[16] batch loss: 0.003\n",
      "[18] batch loss: 0.004\n",
      "[20] batch loss: 0.003\n",
      "[22] batch loss: 0.004\n",
      "[24] batch loss: 0.003\n",
      "[26] batch loss: 0.004\n",
      "[28] batch loss: 0.003\n",
      "epoch 85 loss: 0.096 -> 0.095\n",
      "LC loss: 0.001 -> 0.001\n",
      "DC loss: 0.094 -> 0.093\n",
      "\n",
      "[ 2] batch loss: 0.003\n",
      "[ 4] batch loss: 0.003\n",
      "[ 6] batch loss: 0.003\n"
     ]
    }
   ],
   "source": [
    "prev_loss = np.float(\"inf\")\n",
    "prev_loss_LC = np.float(\"inf\")\n",
    "prev_loss_DC = np.float(\"inf\")\n",
    "total_epoch = 100\n",
    "reset_seq(cnn_da.domain_classifier)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_loss_LC = 0.0\n",
    "    epoch_loss_DC = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    lr = adjust_lr(optimizer, p)\n",
    "    dslr_iter = iter(trainloader_source)\n",
    "    webcam_iter = iter(trainloader_target)\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            images_s, labels_s = dslr_iter.next()\n",
    "            images_t, labels_t = webcam_iter.next()\n",
    "        except:\n",
    "            break\n",
    "        inputs, labels = torch.cat((images_s, images_t)), torch.cat((labels_s, labels_t))\n",
    "        source_size, target_size = labels_s.size(0), labels_t.size(0)\n",
    "        domains = torch.cat((torch.zeros(source_size), torch.ones(target_size)))\n",
    "        if (use_gpu):\n",
    "            inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "        inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, outputs_DC = cnn_da(inputs)\n",
    "        loss_LC = criterion_LC(outputs_LC[:source_size], labels[:source_size])\n",
    "        loss_DC = criterion_DC(outputs_DC.view(-1), domains)\n",
    "        loss = loss_LC + loss_DC\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_loss_LC += loss_LC.item()\n",
    "        epoch_loss_DC += loss_DC.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:    # print every 50 mini-batches\n",
    "            print('[%2d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "        i += 1\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    print(\"LC loss: %.3f -> %.3f\" % (prev_loss_LC, epoch_loss_LC))\n",
    "    print(\"DC loss: %.3f -> %.3f\" % (prev_loss_DC, epoch_loss_DC))\n",
    "    if epoch % 10 == 9:\n",
    "        acc_l, acc_d = evaluate_da_accuracy(cnn_da, trainloader_target, source=False)\n",
    "        print(acc_l, acc_d)\n",
    "    print()\n",
    "    prev_loss = epoch_loss\n",
    "    prev_loss_LC = epoch_loss_LC\n",
    "    prev_loss_DC = epoch_loss_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(cnn_da.state_dict(), \"./parameters/cnn_webcam_to_dslr_0.989.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_da.load_state_dict(torch.load(\"./parameters/cnn_dslr_to_webcam.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

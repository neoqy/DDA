{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp, sqrt\n",
    "from torch.autograd import Variable\n",
    "from my_dataset import MNIST_M\n",
    "from my_dataset import ST_Dataset\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension\n",
    "\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "\n",
    "from data_loader import get_train_test_loader, get_office31_dataloader\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))\n",
    "\n",
    "def reset_seq(seq):\n",
    "    for m in seq:\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, sqrt(2 / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            size = m.weight.size()\n",
    "            fan_out = size[0] # number of rows\n",
    "            fan_in = size[1] # number of columns\n",
    "            m.weight.data.normal_(0, sqrt(2 / (fan_in + fan_out)))\n",
    "            m.bias.data.zero_()\n",
    "        elif hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()\n",
    "            \n",
    "def evaluate_da_accuracy(model, dataloader, source):\n",
    "    model.eval()\n",
    "    correct_LC = 0\n",
    "    correct_DC = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(dataloader):\n",
    "            inputs, labels = data\n",
    "            if (use_gpu):\n",
    "                inputs, labels = inputs.cuda(), labels.cuda()\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            outputs_LC, outputs_DC = model(inputs)\n",
    "            correct_LC += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "            if source:\n",
    "                correct_DC += labels.size()[0] - outputs_DC.data.sum().item()\n",
    "            else:\n",
    "                correct_DC += outputs_DC.data.sum().item()\n",
    "            total += labels.size()[0]\n",
    "        acc_LC = correct_LC / total\n",
    "        acc_DC = correct_DC / total\n",
    "    return acc_LC, acc_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets: webcam\n"
     ]
    }
   ],
   "source": [
    "trainloader_source = get_office31_dataloader(\"webcam\", batch_size=79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading datasets: dslr\n"
     ]
    }
   ],
   "source": [
    "trainloader_target = get_office31_dataloader(\"dslr\", batch_size=49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, lamda):\n",
    "        ctx.save_for_backward(lamda)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        lamda, = ctx.saved_tensors\n",
    "        return -lamda * grad_outputs, None\n",
    "\n",
    "class GRL(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init):\n",
    "        super(GRL, self).__init__()\n",
    "        self.GRL_func = GRL_func.apply\n",
    "        self.lamda = nn.Parameter(torch.Tensor(1), requires_grad=False)\n",
    "        self.set_lamda(lamda_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.GRL_func(x, self.lamda)\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.lamda[0] = lamda_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet_DA(nn.Module):\n",
    "\n",
    "    def __init__(self, lamda_init):\n",
    "        super(AlexNet_DA, self).__init__()\n",
    "        # lambda\n",
    "        self.lamda = lamda_init\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.label_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 31)\n",
    "        )\n",
    "        self.GRL_layer = GRL(lamda_init)\n",
    "        self.domain_classifier = nn.Sequential(\n",
    "            nn.Linear(256, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), 256 * 6 * 6)\n",
    "        x = self.classifier(x)\n",
    "        x_l = self.label_classifier(x)\n",
    "        x_d = self.GRL_layer(x)\n",
    "        x_d = self.domain_classifier(x_d)\n",
    "        return x_l, x_d\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.GRL_layer.set_lamda(lamda_new)\n",
    "    \n",
    "    def load_pretrained_part(self, state_dict):\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                continue\n",
    "            if isinstance(param, nn.Parameter):\n",
    "                # backwards compatibility for serialized parameters\n",
    "                param = param.data\n",
    "            own_state[name].copy_(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet_DA(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (4): ReLU(inplace)\n",
      "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Dropout(p=0.5)\n",
      "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Dropout(p=0.5)\n",
      "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Dropout(p=0.5)\n",
      "    (7): Linear(in_features=4096, out_features=256, bias=True)\n",
      "    (8): ReLU(inplace)\n",
      "  )\n",
      "  (label_classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=31, bias=True)\n",
      "  )\n",
      "  (GRL_layer): GRL()\n",
      "  (domain_classifier): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Linear(in_features=256, out_features=1, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_da = AlexNet_DA(0)\n",
    "\n",
    "model_url = \"https://download.pytorch.org/models/alexnet-owt-4df8aa71.pth\"\n",
    "\n",
    "cnn_da.load_pretrained_part(model_zoo.load_url(model_url))\n",
    "\n",
    "if (use_gpu):\n",
    "    cnn_da.cuda()\n",
    "print(cnn_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in cnn_da.features.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 3.722\n",
      "[ 4] batch loss: 3.051\n",
      "[ 6] batch loss: 2.204\n",
      "[ 8] batch loss: 1.496\n",
      "[10] batch loss: 0.929\n",
      "epoch 1 loss: inf -> 23.945\n",
      "\n",
      "[ 2] batch loss: 0.792\n",
      "[ 4] batch loss: 0.470\n",
      "[ 6] batch loss: 0.454\n",
      "[ 8] batch loss: 0.489\n",
      "[10] batch loss: 0.342\n",
      "epoch 2 loss: 23.945 -> 5.105\n",
      "\n",
      "[ 2] batch loss: 0.279\n",
      "[ 4] batch loss: 0.258\n",
      "[ 6] batch loss: 0.249\n",
      "[ 8] batch loss: 0.175\n",
      "[10] batch loss: 0.201\n",
      "epoch 3 loss: 5.105 -> 2.646\n",
      "\n",
      "[ 2] batch loss: 0.162\n",
      "[ 4] batch loss: 0.175\n",
      "[ 6] batch loss: 0.159\n",
      "[ 8] batch loss: 0.287\n",
      "[10] batch loss: 0.155\n",
      "epoch 4 loss: 2.646 -> 1.878\n",
      "\n",
      "[ 2] batch loss: 0.085\n",
      "[ 4] batch loss: 0.076\n",
      "[ 6] batch loss: 0.085\n",
      "[ 8] batch loss: 0.065\n",
      "[10] batch loss: 0.108\n",
      "epoch 5 loss: 1.878 -> 1.257\n",
      "\n",
      "[ 2] batch loss: 0.079\n",
      "[ 4] batch loss: 0.149\n",
      "[ 6] batch loss: 0.241\n",
      "[ 8] batch loss: 0.171\n",
      "[10] batch loss: 0.185\n",
      "epoch 6 loss: 1.257 -> 2.000\n",
      "\n",
      "[ 2] batch loss: 0.053\n",
      "[ 4] batch loss: 0.138\n",
      "[ 6] batch loss: 0.108\n",
      "[ 8] batch loss: 0.113\n",
      "[10] batch loss: 0.196\n",
      "epoch 7 loss: 2.000 -> 1.220\n",
      "\n",
      "[ 2] batch loss: 0.067\n",
      "[ 4] batch loss: 0.094\n",
      "[ 6] batch loss: 0.068\n",
      "[ 8] batch loss: 0.072\n",
      "[10] batch loss: 0.050\n",
      "epoch 8 loss: 1.220 -> 1.234\n",
      "\n",
      "[ 2] batch loss: 0.057\n",
      "[ 4] batch loss: 0.162\n",
      "[ 6] batch loss: 0.156\n",
      "[ 8] batch loss: 0.099\n",
      "[10] batch loss: 0.228\n",
      "epoch 9 loss: 1.234 -> 1.591\n",
      "\n",
      "[ 2] batch loss: 0.068\n",
      "[ 4] batch loss: 0.051\n",
      "[ 6] batch loss: 0.067\n",
      "[ 8] batch loss: 0.064\n",
      "[10] batch loss: 0.073\n",
      "epoch 10 loss: 1.591 -> 0.649\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 10\n",
    "finetune_epoch_start = 20\n",
    "#reset_seq(cnn_da.classifier)\n",
    "reset_seq(cnn_da.label_classifier)\n",
    "\n",
    "early_stop_cnt = 0\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    lr = adjust_lr(optimizer, p)\n",
    "    if epoch == finetune_epoch_start:\n",
    "        for param in cnn_da.features.parameters():\n",
    "            param.requires_grad = True\n",
    "        optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr, momentum=0.9)\n",
    "    for i, data in enumerate(trainloader_source):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, _ = cnn_da(inputs)\n",
    "        loss = criterion(outputs_LC, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:    # print every 50 mini-batches\n",
    "            print('[%2d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    if prev_loss - epoch_loss < 0.1:\n",
    "        prev_loss = epoch_loss\n",
    "        if epoch > finetune_epoch_start:\n",
    "            early_stop_cnt += 1\n",
    "            if early_stop_cnt > 2:\n",
    "                break\n",
    "    else:\n",
    "        early_stop_cnt = 0\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.0, 0.5158779534153969)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_da_accuracy(cnn_da, trainloader_source, source=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9678714859437751, 0.48369674701767273)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_da_accuracy(cnn_da, trainloader_target, source=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.01\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "\n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in cnn_da.features.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 1.038\n",
      "[ 4] batch loss: 0.926\n",
      "[ 6] batch loss: 1.008\n",
      "[ 8] batch loss: 1.434\n",
      "[10] batch loss: 2.048\n",
      "epoch 1 loss: inf -> 13.695\n",
      "LC loss: inf -> 6.024\n",
      "DC loss: inf -> 7.671\n",
      "\n",
      "[ 2] batch loss: 0.773\n",
      "[ 4] batch loss: 1.270\n",
      "[ 6] batch loss: 0.917\n",
      "[ 8] batch loss: 1.120\n",
      "[10] batch loss: 0.997\n",
      "epoch 2 loss: 13.695 -> 10.838\n",
      "LC loss: 6.024 -> 2.574\n",
      "DC loss: 7.671 -> 8.264\n",
      "\n",
      "[ 2] batch loss: 0.689\n",
      "[ 4] batch loss: 0.762\n",
      "[ 6] batch loss: 0.700\n",
      "[ 8] batch loss: 0.771\n",
      "[10] batch loss: 1.259\n",
      "epoch 3 loss: 10.838 -> 9.065\n",
      "LC loss: 2.574 -> 1.468\n",
      "DC loss: 8.264 -> 7.596\n",
      "\n",
      "[ 2] batch loss: 0.634\n",
      "[ 4] batch loss: 0.742\n",
      "[ 6] batch loss: 0.718\n",
      "[ 8] batch loss: 1.108\n",
      "[10] batch loss: 0.756\n",
      "epoch 4 loss: 9.065 -> 8.946\n",
      "LC loss: 1.468 -> 1.188\n",
      "DC loss: 7.596 -> 7.758\n",
      "\n",
      "[ 2] batch loss: 0.675\n",
      "[ 4] batch loss: 0.704\n",
      "[ 6] batch loss: 0.703\n",
      "[ 8] batch loss: 0.677\n",
      "[10] batch loss: 0.692\n",
      "epoch 5 loss: 8.946 -> 7.549\n",
      "LC loss: 1.188 -> 0.176\n",
      "DC loss: 7.758 -> 7.373\n",
      "\n",
      "[ 2] batch loss: 0.616\n",
      "[ 4] batch loss: 0.663\n",
      "[ 6] batch loss: 0.644\n",
      "[ 8] batch loss: 0.666\n",
      "[10] batch loss: 0.650\n",
      "epoch 6 loss: 7.549 -> 7.093\n",
      "LC loss: 0.176 -> 0.122\n",
      "DC loss: 7.373 -> 6.972\n",
      "\n",
      "[ 2] batch loss: 0.593\n",
      "[ 4] batch loss: 0.640\n",
      "[ 6] batch loss: 0.624\n",
      "[ 8] batch loss: 0.637\n",
      "[10] batch loss: 0.636\n",
      "epoch 7 loss: 7.093 -> 6.823\n",
      "LC loss: 0.122 -> 0.072\n",
      "DC loss: 6.972 -> 6.751\n",
      "\n",
      "[ 2] batch loss: 0.578\n",
      "[ 4] batch loss: 0.628\n",
      "[ 6] batch loss: 0.613\n",
      "[ 8] batch loss: 0.631\n",
      "[10] batch loss: 0.604\n",
      "epoch 8 loss: 6.823 -> 6.637\n",
      "LC loss: 0.072 -> 0.055\n",
      "DC loss: 6.751 -> 6.582\n",
      "\n",
      "[ 2] batch loss: 0.557\n",
      "[ 4] batch loss: 0.616\n",
      "[ 6] batch loss: 0.597\n",
      "[ 8] batch loss: 0.611\n",
      "[10] batch loss: 0.603\n",
      "epoch 9 loss: 6.637 -> 6.434\n",
      "LC loss: 0.055 -> 0.046\n",
      "DC loss: 6.582 -> 6.389\n",
      "\n",
      "[ 2] batch loss: 0.540\n",
      "[ 4] batch loss: 0.604\n",
      "[ 6] batch loss: 0.588\n",
      "[ 8] batch loss: 0.602\n",
      "[10] batch loss: 0.581\n",
      "epoch 10 loss: 6.434 -> 6.245\n",
      "LC loss: 0.046 -> 0.040\n",
      "DC loss: 6.389 -> 6.205\n",
      "0.9819277108433735 0.5254342527274626\n",
      "\n",
      "[ 2] batch loss: 0.532\n",
      "[ 4] batch loss: 0.593\n",
      "[ 6] batch loss: 0.574\n",
      "[ 8] batch loss: 0.586\n",
      "[10] batch loss: 0.563\n",
      "epoch 11 loss: 6.245 -> 6.081\n",
      "LC loss: 0.040 -> 0.036\n",
      "DC loss: 6.205 -> 6.045\n",
      "\n",
      "[ 2] batch loss: 0.505\n",
      "[ 4] batch loss: 0.575\n",
      "[ 6] batch loss: 0.560\n",
      "[ 8] batch loss: 0.561\n",
      "[10] batch loss: 0.574\n",
      "epoch 12 loss: 6.081 -> 5.894\n",
      "LC loss: 0.036 -> 0.033\n",
      "DC loss: 6.045 -> 5.861\n",
      "\n",
      "[ 2] batch loss: 0.513\n",
      "[ 4] batch loss: 0.609\n",
      "[ 6] batch loss: 0.540\n",
      "[ 8] batch loss: 0.565\n",
      "[10] batch loss: 0.586\n",
      "epoch 13 loss: 5.894 -> 5.914\n",
      "LC loss: 0.033 -> 0.030\n",
      "DC loss: 5.861 -> 5.884\n",
      "\n",
      "[ 2] batch loss: 0.585\n",
      "[ 4] batch loss: 0.547\n",
      "[ 6] batch loss: 0.591\n",
      "[ 8] batch loss: 0.644\n",
      "[10] batch loss: 0.532\n",
      "epoch 14 loss: 5.914 -> 6.076\n",
      "LC loss: 0.030 -> 0.028\n",
      "DC loss: 5.884 -> 6.048\n",
      "\n",
      "[ 2] batch loss: 0.494\n",
      "[ 4] batch loss: 0.698\n",
      "[ 6] batch loss: 0.615\n",
      "[ 8] batch loss: 0.542\n",
      "[10] batch loss: 0.547\n",
      "epoch 15 loss: 6.076 -> 6.188\n",
      "LC loss: 0.028 -> 0.026\n",
      "DC loss: 6.048 -> 6.161\n",
      "\n",
      "[ 2] batch loss: 0.653\n",
      "[ 4] batch loss: 0.593\n",
      "[ 6] batch loss: 0.554\n",
      "[ 8] batch loss: 0.590\n",
      "[10] batch loss: 0.703\n",
      "epoch 16 loss: 6.188 -> 7.038\n",
      "LC loss: 0.026 -> 0.025\n",
      "DC loss: 6.161 -> 7.013\n",
      "\n",
      "[ 2] batch loss: 0.532\n",
      "[ 4] batch loss: 0.580\n",
      "[ 6] batch loss: 0.616\n",
      "[ 8] batch loss: 0.630\n",
      "[10] batch loss: 0.546\n",
      "epoch 17 loss: 7.038 -> 6.506\n",
      "LC loss: 0.025 -> 0.024\n",
      "DC loss: 7.013 -> 6.483\n",
      "\n",
      "[ 2] batch loss: 0.518\n",
      "[ 4] batch loss: 0.572\n",
      "[ 6] batch loss: 0.564\n",
      "[ 8] batch loss: 0.558\n",
      "[10] batch loss: 0.577\n",
      "epoch 18 loss: 6.506 -> 6.018\n",
      "LC loss: 0.024 -> 0.023\n",
      "DC loss: 6.483 -> 5.996\n",
      "\n",
      "[ 2] batch loss: 0.477\n",
      "[ 4] batch loss: 0.552\n",
      "[ 6] batch loss: 0.533\n",
      "[ 8] batch loss: 0.537\n",
      "[10] batch loss: 0.535\n",
      "epoch 19 loss: 6.018 -> 5.599\n",
      "LC loss: 0.023 -> 0.022\n",
      "DC loss: 5.996 -> 5.577\n",
      "\n",
      "[ 2] batch loss: 0.450\n",
      "[ 4] batch loss: 0.523\n",
      "[ 6] batch loss: 0.501\n",
      "[ 8] batch loss: 0.500\n",
      "[10] batch loss: 0.505\n",
      "epoch 20 loss: 5.599 -> 5.251\n",
      "LC loss: 0.022 -> 0.021\n",
      "DC loss: 5.577 -> 5.230\n",
      "0.9819277108433735 0.5462798526488155\n",
      "\n",
      "[ 2] batch loss: 0.426\n",
      "[ 4] batch loss: 0.498\n",
      "[ 6] batch loss: 0.478\n",
      "[ 8] batch loss: 0.474\n",
      "[10] batch loss: 0.508\n",
      "epoch 21 loss: 5.251 -> 4.977\n",
      "LC loss: 0.021 -> 0.020\n",
      "DC loss: 5.230 -> 4.957\n",
      "\n",
      "[ 2] batch loss: 0.416\n",
      "[ 4] batch loss: 0.494\n",
      "[ 6] batch loss: 0.455\n",
      "[ 8] batch loss: 0.462\n",
      "[10] batch loss: 0.491\n",
      "epoch 22 loss: 4.977 -> 4.797\n",
      "LC loss: 0.020 -> 0.019\n",
      "DC loss: 4.957 -> 4.778\n",
      "\n",
      "[ 2] batch loss: 0.422\n",
      "[ 4] batch loss: 0.454\n",
      "[ 6] batch loss: 0.460\n",
      "[ 8] batch loss: 0.486\n",
      "[10] batch loss: 0.450\n",
      "epoch 23 loss: 4.797 -> 4.681\n",
      "LC loss: 0.019 -> 0.019\n",
      "DC loss: 4.778 -> 4.662\n",
      "\n",
      "[ 2] batch loss: 0.410\n",
      "[ 4] batch loss: 0.517\n",
      "[ 6] batch loss: 0.450\n",
      "[ 8] batch loss: 0.488\n",
      "[10] batch loss: 0.461\n",
      "epoch 24 loss: 4.681 -> 4.852\n",
      "LC loss: 0.019 -> 0.018\n",
      "DC loss: 4.662 -> 4.834\n",
      "\n",
      "[ 2] batch loss: 0.419\n",
      "[ 4] batch loss: 0.471\n",
      "[ 6] batch loss: 0.633\n",
      "[ 8] batch loss: 0.531\n",
      "[10] batch loss: 0.480\n",
      "epoch 25 loss: 4.852 -> 5.262\n",
      "LC loss: 0.018 -> 0.018\n",
      "DC loss: 4.834 -> 5.244\n",
      "\n",
      "[ 2] batch loss: 0.813\n",
      "[ 4] batch loss: 1.184\n",
      "[ 6] batch loss: 0.638\n",
      "[ 8] batch loss: 0.519\n",
      "[10] batch loss: 0.679\n",
      "epoch 26 loss: 5.262 -> 8.283\n",
      "LC loss: 0.018 -> 0.017\n",
      "DC loss: 5.244 -> 8.266\n",
      "\n",
      "[ 2] batch loss: 0.957\n",
      "[ 4] batch loss: 0.613\n",
      "[ 6] batch loss: 0.556\n",
      "[ 8] batch loss: 0.575\n",
      "[10] batch loss: 0.566\n",
      "epoch 27 loss: 8.283 -> 7.539\n",
      "LC loss: 0.017 -> 0.017\n",
      "DC loss: 8.266 -> 7.522\n",
      "\n",
      "[ 2] batch loss: 0.536\n",
      "[ 4] batch loss: 0.586\n",
      "[ 6] batch loss: 0.562\n",
      "[ 8] batch loss: 0.566\n",
      "[10] batch loss: 0.561\n",
      "epoch 28 loss: 7.539 -> 6.139\n",
      "LC loss: 0.017 -> 0.016\n",
      "DC loss: 7.522 -> 6.123\n",
      "\n",
      "[ 2] batch loss: 0.527\n",
      "[ 4] batch loss: 0.560\n",
      "[ 6] batch loss: 0.533\n",
      "[ 8] batch loss: 0.525\n",
      "[10] batch loss: 0.537\n",
      "epoch 29 loss: 6.139 -> 5.766\n",
      "LC loss: 0.016 -> 0.016\n",
      "DC loss: 6.123 -> 5.750\n",
      "\n",
      "[ 2] batch loss: 0.492\n",
      "[ 4] batch loss: 0.529\n",
      "[ 6] batch loss: 0.502\n",
      "[ 8] batch loss: 0.494\n",
      "[10] batch loss: 0.499\n",
      "epoch 30 loss: 5.766 -> 5.339\n",
      "LC loss: 0.016 -> 0.015\n",
      "DC loss: 5.750 -> 5.323\n",
      "0.9839357429718876 0.5327356292540768\n",
      "\n",
      "[ 2] batch loss: 0.460\n",
      "[ 4] batch loss: 0.501\n",
      "[ 6] batch loss: 0.475\n",
      "[ 8] batch loss: 0.464\n",
      "[10] batch loss: 0.477\n",
      "epoch 31 loss: 5.339 -> 4.972\n",
      "LC loss: 0.015 -> 0.015\n",
      "DC loss: 5.323 -> 4.957\n",
      "\n",
      "[ 2] batch loss: 0.434\n",
      "[ 4] batch loss: 0.473\n",
      "[ 6] batch loss: 0.450\n",
      "[ 8] batch loss: 0.439\n",
      "[10] batch loss: 0.454\n",
      "epoch 32 loss: 4.972 -> 4.675\n",
      "LC loss: 0.015 -> 0.015\n",
      "DC loss: 4.957 -> 4.660\n",
      "\n",
      "[ 2] batch loss: 0.400\n",
      "[ 4] batch loss: 0.449\n",
      "[ 6] batch loss: 0.423\n",
      "[ 8] batch loss: 0.412\n",
      "[10] batch loss: 0.438\n",
      "epoch 33 loss: 4.675 -> 4.376\n",
      "LC loss: 0.015 -> 0.014\n",
      "DC loss: 4.660 -> 4.361\n",
      "\n",
      "[ 2] batch loss: 0.374\n",
      "[ 4] batch loss: 0.426\n",
      "[ 6] batch loss: 0.400\n",
      "[ 8] batch loss: 0.391\n",
      "[10] batch loss: 0.423\n",
      "epoch 34 loss: 4.376 -> 4.131\n",
      "LC loss: 0.014 -> 0.014\n",
      "DC loss: 4.361 -> 4.117\n",
      "\n",
      "[ 2] batch loss: 0.356\n",
      "[ 4] batch loss: 0.403\n",
      "[ 6] batch loss: 0.380\n",
      "[ 8] batch loss: 0.375\n",
      "[10] batch loss: 0.400\n",
      "epoch 35 loss: 4.131 -> 3.918\n",
      "LC loss: 0.014 -> 0.014\n",
      "DC loss: 4.117 -> 3.904\n",
      "\n",
      "[ 2] batch loss: 0.336\n",
      "[ 4] batch loss: 0.386\n",
      "[ 6] batch loss: 0.361\n",
      "[ 8] batch loss: 0.356\n",
      "[10] batch loss: 0.383\n",
      "epoch 36 loss: 3.918 -> 3.719\n",
      "LC loss: 0.014 -> 0.014\n",
      "DC loss: 3.904 -> 3.705\n",
      "\n",
      "[ 2] batch loss: 0.317\n",
      "[ 4] batch loss: 0.367\n",
      "[ 6] batch loss: 0.342\n",
      "[ 8] batch loss: 0.341\n",
      "[10] batch loss: 0.375\n",
      "epoch 37 loss: 3.719 -> 3.547\n",
      "LC loss: 0.014 -> 0.013\n",
      "DC loss: 3.705 -> 3.534\n",
      "\n",
      "[ 2] batch loss: 0.311\n",
      "[ 4] batch loss: 0.370\n",
      "[ 6] batch loss: 0.321\n",
      "[ 8] batch loss: 0.334\n",
      "[10] batch loss: 0.381\n",
      "epoch 38 loss: 3.547 -> 3.493\n",
      "LC loss: 0.013 -> 0.013\n",
      "DC loss: 3.534 -> 3.480\n",
      "\n",
      "[ 2] batch loss: 0.321\n",
      "[ 4] batch loss: 0.346\n",
      "[ 6] batch loss: 0.351\n",
      "[ 8] batch loss: 0.373\n",
      "[10] batch loss: 0.352\n",
      "epoch 39 loss: 3.493 -> 3.538\n",
      "LC loss: 0.013 -> 0.013\n",
      "DC loss: 3.480 -> 3.525\n",
      "\n",
      "[ 2] batch loss: 0.385\n",
      "[ 4] batch loss: 0.495\n",
      "[ 6] batch loss: 0.358\n",
      "[ 8] batch loss: 0.526\n",
      "[10] batch loss: 0.468\n",
      "epoch 40 loss: 3.538 -> 4.683\n",
      "LC loss: 0.013 -> 0.013\n",
      "DC loss: 3.525 -> 4.671\n",
      "0.9859437751004017 0.6465778867882418\n",
      "\n",
      "[ 2] batch loss: 0.394\n",
      "[ 4] batch loss: 0.494\n",
      "[ 6] batch loss: 0.863\n",
      "[ 8] batch loss: 0.685\n",
      "[10] batch loss: 0.427\n",
      "epoch 41 loss: 4.683 -> 5.942\n",
      "LC loss: 0.013 -> 0.012\n",
      "DC loss: 4.671 -> 5.930\n",
      "\n",
      "[ 2] batch loss: 0.941\n",
      "[ 4] batch loss: 1.556\n",
      "[ 6] batch loss: 1.174\n",
      "[ 8] batch loss: 0.584\n",
      "[10] batch loss: 0.481\n",
      "epoch 42 loss: 5.942 -> 9.793\n",
      "LC loss: 0.012 -> 0.012\n",
      "DC loss: 5.930 -> 9.781\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2] batch loss: 0.686\n",
      "[ 4] batch loss: 0.553\n",
      "[ 6] batch loss: 0.514\n",
      "[ 8] batch loss: 0.570\n",
      "[10] batch loss: 0.513\n",
      "epoch 43 loss: 9.793 -> 6.028\n",
      "LC loss: 0.012 -> 0.012\n",
      "DC loss: 9.781 -> 6.016\n",
      "\n",
      "[ 2] batch loss: 0.513\n",
      "[ 4] batch loss: 0.499\n",
      "[ 6] batch loss: 0.485\n",
      "[ 8] batch loss: 0.497\n",
      "[10] batch loss: 0.465\n",
      "epoch 44 loss: 6.028 -> 5.159\n",
      "LC loss: 0.012 -> 0.012\n",
      "DC loss: 6.016 -> 5.147\n",
      "\n",
      "[ 2] batch loss: 0.442\n",
      "[ 4] batch loss: 0.480\n",
      "[ 6] batch loss: 0.447\n",
      "[ 8] batch loss: 0.461\n",
      "[10] batch loss: 0.441\n",
      "epoch 45 loss: 5.159 -> 4.694\n",
      "LC loss: 0.012 -> 0.012\n",
      "DC loss: 5.147 -> 4.682\n",
      "\n",
      "[ 2] batch loss: 0.413\n",
      "[ 4] batch loss: 0.441\n",
      "[ 6] batch loss: 0.412\n",
      "[ 8] batch loss: 0.420\n",
      "[10] batch loss: 0.401\n",
      "epoch 46 loss: 4.694 -> 4.271\n",
      "LC loss: 0.012 -> 0.012\n",
      "DC loss: 4.682 -> 4.260\n",
      "\n",
      "[ 2] batch loss: 0.383\n",
      "[ 4] batch loss: 0.408\n",
      "[ 6] batch loss: 0.380\n",
      "[ 8] batch loss: 0.382\n",
      "[10] batch loss: 0.379\n",
      "epoch 47 loss: 4.271 -> 3.941\n",
      "LC loss: 0.012 -> 0.011\n",
      "DC loss: 4.260 -> 3.930\n",
      "\n",
      "[ 2] batch loss: 0.355\n",
      "[ 4] batch loss: 0.381\n",
      "[ 6] batch loss: 0.353\n",
      "[ 8] batch loss: 0.353\n",
      "[10] batch loss: 0.358\n",
      "epoch 48 loss: 3.941 -> 3.661\n",
      "LC loss: 0.011 -> 0.011\n",
      "DC loss: 3.930 -> 3.650\n",
      "\n",
      "[ 2] batch loss: 0.325\n",
      "[ 4] batch loss: 0.362\n",
      "[ 6] batch loss: 0.329\n",
      "[ 8] batch loss: 0.330\n",
      "[10] batch loss: 0.341\n",
      "epoch 49 loss: 3.661 -> 3.423\n",
      "LC loss: 0.011 -> 0.011\n",
      "DC loss: 3.650 -> 3.412\n",
      "\n",
      "[ 2] batch loss: 0.303\n",
      "[ 4] batch loss: 0.342\n",
      "[ 6] batch loss: 0.309\n",
      "[ 8] batch loss: 0.310\n",
      "[10] batch loss: 0.324\n",
      "epoch 50 loss: 3.423 -> 3.219\n",
      "LC loss: 0.011 -> 0.011\n",
      "DC loss: 3.412 -> 3.208\n",
      "0.9879518072289156 0.7048392382012793\n",
      "\n",
      "[ 2] batch loss: 0.284\n",
      "[ 4] batch loss: 0.323\n",
      "[ 6] batch loss: 0.292\n",
      "[ 8] batch loss: 0.292\n",
      "[10] batch loss: 0.311\n",
      "epoch 51 loss: 3.219 -> 3.041\n",
      "LC loss: 0.011 -> 0.011\n",
      "DC loss: 3.208 -> 3.030\n",
      "\n",
      "[ 2] batch loss: 0.268\n",
      "[ 4] batch loss: 0.308\n",
      "[ 6] batch loss: 0.277\n",
      "[ 8] batch loss: 0.278\n",
      "[10] batch loss: 0.299\n",
      "epoch 52 loss: 3.041 -> 2.890\n",
      "LC loss: 0.011 -> 0.011\n",
      "DC loss: 3.030 -> 2.880\n",
      "\n",
      "[ 2] batch loss: 0.256\n",
      "[ 4] batch loss: 0.293\n",
      "[ 6] batch loss: 0.262\n",
      "[ 8] batch loss: 0.268\n",
      "[10] batch loss: 0.287\n",
      "epoch 53 loss: 2.890 -> 2.759\n",
      "LC loss: 0.011 -> 0.011\n",
      "DC loss: 2.880 -> 2.749\n",
      "\n",
      "[ 2] batch loss: 0.244\n",
      "[ 4] batch loss: 0.279\n",
      "[ 6] batch loss: 0.251\n",
      "[ 8] batch loss: 0.256\n",
      "[10] batch loss: 0.275\n",
      "epoch 54 loss: 2.759 -> 2.636\n",
      "LC loss: 0.011 -> 0.010\n",
      "DC loss: 2.749 -> 2.625\n",
      "\n",
      "[ 2] batch loss: 0.230\n",
      "[ 4] batch loss: 0.268\n",
      "[ 6] batch loss: 0.241\n",
      "[ 8] batch loss: 0.246\n",
      "[10] batch loss: 0.268\n",
      "epoch 55 loss: 2.636 -> 2.526\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 2.625 -> 2.516\n",
      "\n",
      "[ 2] batch loss: 0.225\n",
      "[ 4] batch loss: 0.269\n",
      "[ 6] batch loss: 0.227\n",
      "[ 8] batch loss: 0.241\n",
      "[10] batch loss: 0.269\n",
      "epoch 56 loss: 2.526 -> 2.482\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 2.516 -> 2.472\n",
      "\n",
      "[ 2] batch loss: 0.232\n",
      "[ 4] batch loss: 0.251\n",
      "[ 6] batch loss: 0.236\n",
      "[ 8] batch loss: 0.261\n",
      "[10] batch loss: 0.251\n",
      "epoch 57 loss: 2.482 -> 2.478\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 2.472 -> 2.468\n",
      "\n",
      "[ 2] batch loss: 0.232\n",
      "[ 4] batch loss: 0.317\n",
      "[ 6] batch loss: 0.249\n",
      "[ 8] batch loss: 0.254\n",
      "[10] batch loss: 0.299\n",
      "epoch 58 loss: 2.478 -> 2.772\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 2.468 -> 2.762\n",
      "\n",
      "[ 2] batch loss: 0.288\n",
      "[ 4] batch loss: 0.252\n",
      "[ 6] batch loss: 0.424\n",
      "[ 8] batch loss: 0.466\n",
      "[10] batch loss: 0.303\n",
      "epoch 59 loss: 2.772 -> 3.486\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 2.762 -> 3.476\n",
      "\n",
      "[ 2] batch loss: 0.544\n",
      "[ 4] batch loss: 1.134\n",
      "[ 6] batch loss: 0.693\n",
      "[ 8] batch loss: 0.341\n",
      "[10] batch loss: 0.762\n",
      "epoch 60 loss: 3.486 -> 8.150\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 3.476 -> 8.140\n",
      "0.9899598393574297 0.9208751940822985\n",
      "\n",
      "[ 2] batch loss: 1.174\n",
      "[ 4] batch loss: 0.447\n",
      "[ 6] batch loss: 0.551\n",
      "[ 8] batch loss: 0.874\n",
      "[10] batch loss: 0.868\n",
      "epoch 61 loss: 8.150 -> 10.822\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 8.140 -> 10.812\n",
      "\n",
      "[ 2] batch loss: 0.420\n",
      "[ 4] batch loss: 0.526\n",
      "[ 6] batch loss: 0.575\n",
      "[ 8] batch loss: 0.590\n",
      "[10] batch loss: 0.521\n",
      "epoch 62 loss: 10.822 -> 5.641\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 10.812 -> 5.632\n",
      "\n",
      "[ 2] batch loss: 0.418\n",
      "[ 4] batch loss: 0.496\n",
      "[ 6] batch loss: 0.458\n",
      "[ 8] batch loss: 0.455\n",
      "[10] batch loss: 0.472\n",
      "epoch 63 loss: 5.641 -> 4.922\n",
      "LC loss: 0.010 -> 0.010\n",
      "DC loss: 5.632 -> 4.913\n",
      "\n",
      "[ 2] batch loss: 0.417\n",
      "[ 4] batch loss: 0.466\n",
      "[ 6] batch loss: 0.418\n",
      "[ 8] batch loss: 0.418\n",
      "[10] batch loss: 0.428\n",
      "epoch 64 loss: 4.922 -> 4.515\n",
      "LC loss: 0.010 -> 0.009\n",
      "DC loss: 4.913 -> 4.506\n",
      "\n",
      "[ 2] batch loss: 0.363\n",
      "[ 4] batch loss: 0.408\n",
      "[ 6] batch loss: 0.378\n",
      "[ 8] batch loss: 0.373\n",
      "[10] batch loss: 0.390\n",
      "epoch 65 loss: 4.515 -> 3.988\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 4.506 -> 3.979\n",
      "\n",
      "[ 2] batch loss: 0.335\n",
      "[ 4] batch loss: 0.376\n",
      "[ 6] batch loss: 0.341\n",
      "[ 8] batch loss: 0.339\n",
      "[10] batch loss: 0.356\n",
      "epoch 66 loss: 3.988 -> 3.623\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 3.979 -> 3.614\n",
      "\n",
      "[ 2] batch loss: 0.307\n",
      "[ 4] batch loss: 0.352\n",
      "[ 6] batch loss: 0.311\n",
      "[ 8] batch loss: 0.308\n",
      "[10] batch loss: 0.331\n",
      "epoch 67 loss: 3.623 -> 3.317\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 3.614 -> 3.308\n",
      "\n",
      "[ 2] batch loss: 0.281\n",
      "[ 4] batch loss: 0.330\n",
      "[ 6] batch loss: 0.285\n",
      "[ 8] batch loss: 0.284\n",
      "[10] batch loss: 0.311\n",
      "epoch 68 loss: 3.317 -> 3.061\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 3.308 -> 3.052\n",
      "\n",
      "[ 2] batch loss: 0.261\n",
      "[ 4] batch loss: 0.308\n",
      "[ 6] batch loss: 0.266\n",
      "[ 8] batch loss: 0.263\n",
      "[10] batch loss: 0.294\n",
      "epoch 69 loss: 3.061 -> 2.849\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 3.052 -> 2.839\n",
      "\n",
      "[ 2] batch loss: 0.244\n",
      "[ 4] batch loss: 0.289\n",
      "[ 6] batch loss: 0.248\n",
      "[ 8] batch loss: 0.246\n",
      "[10] batch loss: 0.280\n",
      "epoch 70 loss: 2.849 -> 2.668\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 2.839 -> 2.659\n",
      "0.9899598393574297 0.7541982313715311\n",
      "\n",
      "[ 2] batch loss: 0.230\n",
      "[ 4] batch loss: 0.272\n",
      "[ 6] batch loss: 0.233\n",
      "[ 8] batch loss: 0.233\n",
      "[10] batch loss: 0.267\n",
      "epoch 71 loss: 2.668 -> 2.513\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 2.659 -> 2.504\n",
      "\n",
      "[ 2] batch loss: 0.216\n",
      "[ 4] batch loss: 0.257\n",
      "[ 6] batch loss: 0.219\n",
      "[ 8] batch loss: 0.220\n",
      "[10] batch loss: 0.256\n",
      "epoch 72 loss: 2.513 -> 2.374\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 2.504 -> 2.366\n",
      "\n",
      "[ 2] batch loss: 0.205\n",
      "[ 4] batch loss: 0.245\n",
      "[ 6] batch loss: 0.207\n",
      "[ 8] batch loss: 0.211\n",
      "[10] batch loss: 0.246\n",
      "epoch 73 loss: 2.374 -> 2.258\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 2.366 -> 2.249\n",
      "\n",
      "[ 2] batch loss: 0.195\n",
      "[ 4] batch loss: 0.233\n",
      "[ 6] batch loss: 0.198\n",
      "[ 8] batch loss: 0.202\n",
      "[10] batch loss: 0.235\n",
      "epoch 74 loss: 2.258 -> 2.155\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 2.249 -> 2.147\n",
      "\n",
      "[ 2] batch loss: 0.186\n",
      "[ 4] batch loss: 0.223\n",
      "[ 6] batch loss: 0.190\n",
      "[ 8] batch loss: 0.194\n",
      "[10] batch loss: 0.227\n",
      "epoch 75 loss: 2.155 -> 2.062\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 2.147 -> 2.054\n",
      "\n",
      "[ 2] batch loss: 0.178\n",
      "[ 4] batch loss: 0.216\n",
      "[ 6] batch loss: 0.180\n",
      "[ 8] batch loss: 0.188\n",
      "[10] batch loss: 0.220\n",
      "epoch 76 loss: 2.062 -> 1.985\n",
      "LC loss: 0.009 -> 0.009\n",
      "DC loss: 2.054 -> 1.976\n",
      "\n",
      "[ 2] batch loss: 0.175\n",
      "[ 4] batch loss: 0.206\n",
      "[ 6] batch loss: 0.173\n",
      "[ 8] batch loss: 0.186\n",
      "[10] batch loss: 0.210\n",
      "epoch 77 loss: 1.985 -> 1.919\n",
      "LC loss: 0.009 -> 0.008\n",
      "DC loss: 1.976 -> 1.911\n",
      "\n",
      "[ 2] batch loss: 0.165\n",
      "[ 4] batch loss: 0.202\n",
      "[ 6] batch loss: 0.173\n",
      "[ 8] batch loss: 0.175\n",
      "[10] batch loss: 0.204\n",
      "epoch 78 loss: 1.919 -> 1.860\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.911 -> 1.852\n",
      "\n",
      "[ 2] batch loss: 0.164\n",
      "[ 4] batch loss: 0.203\n",
      "[ 6] batch loss: 0.161\n",
      "[ 8] batch loss: 0.188\n",
      "[10] batch loss: 0.210\n",
      "epoch 79 loss: 1.860 -> 1.868\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.852 -> 1.860\n",
      "\n",
      "[ 2] batch loss: 0.179\n",
      "[ 4] batch loss: 0.211\n",
      "[ 6] batch loss: 0.188\n",
      "[ 8] batch loss: 0.189\n",
      "[10] batch loss: 0.198\n",
      "epoch 80 loss: 1.868 -> 1.951\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.860 -> 1.943\n",
      "0.9899598393574297 0.9153365227113287\n",
      "\n",
      "[ 2] batch loss: 0.214\n",
      "[ 4] batch loss: 0.274\n",
      "[ 6] batch loss: 0.172\n",
      "[ 8] batch loss: 0.258\n",
      "[10] batch loss: 0.254\n",
      "epoch 81 loss: 1.951 -> 2.396\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.943 -> 2.387\n",
      "\n",
      "[ 2] batch loss: 0.270\n",
      "[ 4] batch loss: 0.248\n",
      "[ 6] batch loss: 0.412\n",
      "[ 8] batch loss: 0.363\n",
      "[10] batch loss: 0.206\n",
      "epoch 82 loss: 2.396 -> 3.019\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 2.387 -> 3.011\n",
      "\n",
      "[ 2] batch loss: 0.445\n",
      "[ 4] batch loss: 0.950\n",
      "[ 6] batch loss: 0.534\n",
      "[ 8] batch loss: 0.306\n",
      "[10] batch loss: 0.581\n",
      "epoch 83 loss: 3.019 -> 6.198\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 3.011 -> 6.190\n",
      "\n",
      "[ 2] batch loss: 1.095\n",
      "[ 4] batch loss: 0.411\n",
      "[ 6] batch loss: 0.425\n",
      "[ 8] batch loss: 0.860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10] batch loss: 0.615\n",
      "epoch 84 loss: 6.198 -> 7.637\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 6.190 -> 7.629\n",
      "\n",
      "[ 2] batch loss: 0.317\n",
      "[ 4] batch loss: 0.448\n",
      "[ 6] batch loss: 0.478\n",
      "[ 8] batch loss: 0.495\n",
      "[10] batch loss: 0.310\n",
      "epoch 85 loss: 7.637 -> 4.192\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 7.629 -> 4.184\n",
      "\n",
      "[ 2] batch loss: 0.263\n",
      "[ 4] batch loss: 0.368\n",
      "[ 6] batch loss: 0.286\n",
      "[ 8] batch loss: 0.305\n",
      "[10] batch loss: 0.272\n",
      "epoch 86 loss: 4.192 -> 3.091\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 4.184 -> 3.083\n",
      "\n",
      "[ 2] batch loss: 0.249\n",
      "[ 4] batch loss: 0.273\n",
      "[ 6] batch loss: 0.233\n",
      "[ 8] batch loss: 0.245\n",
      "[10] batch loss: 0.255\n",
      "epoch 87 loss: 3.091 -> 2.575\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 3.083 -> 2.568\n",
      "\n",
      "[ 2] batch loss: 0.228\n",
      "[ 4] batch loss: 0.247\n",
      "[ 6] batch loss: 0.208\n",
      "[ 8] batch loss: 0.211\n",
      "[10] batch loss: 0.230\n",
      "epoch 88 loss: 2.575 -> 2.300\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 2.568 -> 2.293\n",
      "\n",
      "[ 2] batch loss: 0.197\n",
      "[ 4] batch loss: 0.227\n",
      "[ 6] batch loss: 0.185\n",
      "[ 8] batch loss: 0.189\n",
      "[10] batch loss: 0.215\n",
      "epoch 89 loss: 2.300 -> 2.068\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 2.293 -> 2.060\n",
      "\n",
      "[ 2] batch loss: 0.178\n",
      "[ 4] batch loss: 0.213\n",
      "[ 6] batch loss: 0.166\n",
      "[ 8] batch loss: 0.177\n",
      "[10] batch loss: 0.202\n",
      "epoch 90 loss: 2.068 -> 1.906\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 2.060 -> 1.898\n",
      "0.9899598393574297 0.8182384192225445\n",
      "\n",
      "[ 2] batch loss: 0.167\n",
      "[ 4] batch loss: 0.197\n",
      "[ 6] batch loss: 0.156\n",
      "[ 8] batch loss: 0.166\n",
      "[10] batch loss: 0.192\n",
      "epoch 91 loss: 1.906 -> 1.782\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.898 -> 1.775\n",
      "\n",
      "[ 2] batch loss: 0.156\n",
      "[ 4] batch loss: 0.186\n",
      "[ 6] batch loss: 0.147\n",
      "[ 8] batch loss: 0.158\n",
      "[10] batch loss: 0.184\n",
      "epoch 92 loss: 1.782 -> 1.685\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.775 -> 1.677\n",
      "\n",
      "[ 2] batch loss: 0.148\n",
      "[ 4] batch loss: 0.177\n",
      "[ 6] batch loss: 0.139\n",
      "[ 8] batch loss: 0.152\n",
      "[10] batch loss: 0.176\n",
      "epoch 93 loss: 1.685 -> 1.604\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.677 -> 1.596\n",
      "\n",
      "[ 2] batch loss: 0.140\n",
      "[ 4] batch loss: 0.169\n",
      "[ 6] batch loss: 0.133\n",
      "[ 8] batch loss: 0.146\n",
      "[10] batch loss: 0.170\n",
      "epoch 94 loss: 1.604 -> 1.535\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.596 -> 1.527\n",
      "\n",
      "[ 2] batch loss: 0.134\n",
      "[ 4] batch loss: 0.163\n",
      "[ 6] batch loss: 0.127\n",
      "[ 8] batch loss: 0.141\n",
      "[10] batch loss: 0.165\n",
      "epoch 95 loss: 1.535 -> 1.476\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.527 -> 1.469\n",
      "\n",
      "[ 2] batch loss: 0.131\n",
      "[ 4] batch loss: 0.156\n",
      "[ 6] batch loss: 0.122\n",
      "[ 8] batch loss: 0.137\n",
      "[10] batch loss: 0.159\n",
      "epoch 96 loss: 1.476 -> 1.425\n",
      "LC loss: 0.008 -> 0.008\n",
      "DC loss: 1.469 -> 1.418\n",
      "\n",
      "[ 2] batch loss: 0.124\n",
      "[ 4] batch loss: 0.152\n",
      "[ 6] batch loss: 0.119\n",
      "[ 8] batch loss: 0.132\n",
      "[10] batch loss: 0.154\n",
      "epoch 97 loss: 1.425 -> 1.376\n",
      "LC loss: 0.008 -> 0.007\n",
      "DC loss: 1.418 -> 1.368\n",
      "\n",
      "[ 2] batch loss: 0.120\n",
      "[ 4] batch loss: 0.150\n",
      "[ 6] batch loss: 0.113\n",
      "[ 8] batch loss: 0.129\n",
      "[10] batch loss: 0.151\n",
      "epoch 98 loss: 1.376 -> 1.340\n",
      "LC loss: 0.007 -> 0.007\n",
      "DC loss: 1.368 -> 1.332\n",
      "\n",
      "[ 2] batch loss: 0.123\n",
      "[ 4] batch loss: 0.143\n",
      "[ 6] batch loss: 0.110\n",
      "[ 8] batch loss: 0.130\n",
      "[10] batch loss: 0.145\n",
      "epoch 99 loss: 1.340 -> 1.312\n",
      "LC loss: 0.007 -> 0.007\n",
      "DC loss: 1.332 -> 1.305\n",
      "\n",
      "[ 2] batch loss: 0.116\n",
      "[ 4] batch loss: 0.144\n",
      "[ 6] batch loss: 0.113\n",
      "[ 8] batch loss: 0.122\n",
      "[10] batch loss: 0.141\n",
      "epoch 100 loss: 1.312 -> 1.282\n",
      "LC loss: 0.007 -> 0.007\n",
      "DC loss: 1.305 -> 1.275\n",
      "0.9899598393574297 0.8273351929752703\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prev_loss = np.float(\"inf\")\n",
    "prev_loss_LC = np.float(\"inf\")\n",
    "prev_loss_DC = np.float(\"inf\")\n",
    "total_epoch = 100\n",
    "reset_seq(cnn_da.domain_classifier)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    epoch_loss_LC = 0.0\n",
    "    epoch_loss_DC = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    lr = adjust_lr(optimizer, p)\n",
    "    dslr_iter = iter(trainloader_source)\n",
    "    webcam_iter = iter(trainloader_target)\n",
    "    i = 0\n",
    "    while True:\n",
    "        try:\n",
    "            images_s, labels_s = dslr_iter.next()\n",
    "            images_t, labels_t = webcam_iter.next()\n",
    "        except:\n",
    "            break\n",
    "        inputs, labels = torch.cat((images_s, images_t)), torch.cat((labels_s, labels_t))\n",
    "        source_size, target_size = labels_s.size(0), labels_t.size(0)\n",
    "        domains = torch.cat((torch.zeros(source_size), torch.ones(target_size)))\n",
    "        if (use_gpu):\n",
    "            inputs, labels, domains = inputs.cuda(), labels.cuda(), domains.cuda()\n",
    "        inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, outputs_DC = cnn_da(inputs)\n",
    "        loss_LC = criterion_LC(outputs_LC[:source_size], labels[:source_size])\n",
    "        loss_DC = criterion_DC(outputs_DC.view(-1), domains)\n",
    "        loss = loss_LC + loss_DC\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_loss_LC += loss_LC.item()\n",
    "        epoch_loss_DC += loss_DC.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2 == 1:    # print every 50 mini-batches\n",
    "            print('[%2d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 2))\n",
    "            running_loss = 0.0\n",
    "        i += 1\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    print(\"LC loss: %.3f -> %.3f\" % (prev_loss_LC, epoch_loss_LC))\n",
    "    print(\"DC loss: %.3f -> %.3f\" % (prev_loss_DC, epoch_loss_DC))\n",
    "    if epoch % 10 == 9:\n",
    "        acc_l, acc_d = evaluate_da_accuracy(cnn_da, trainloader_target, source=False)\n",
    "        print(acc_l, acc_d)\n",
    "    print()\n",
    "    prev_loss = epoch_loss\n",
    "    prev_loss_LC = epoch_loss_LC\n",
    "    prev_loss_DC = epoch_loss_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(cnn_da.state_dict(), \"./parameters/cnn_webcam_to_dslr_0.989.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn_da.load_state_dict(torch.load(\"./parameters/cnn_dslr_to_webcam.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

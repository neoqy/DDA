{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RTya_dtvpdaX"
   },
   "source": [
    "# Preparations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "e9W0cTFEPmAl"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import manifold\n",
    "from math import exp, sqrt\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from my_dataset import ST_Dataset\n",
    "from my_dataset import SYNSIGN\n",
    "from my_dataset import GTSRB\n",
    "%matplotlib inline\n",
    "%load_ext skip_kernel_extension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 363,
     "status": "ok",
     "timestamp": 1523506225267,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "4BrzNF_oQL77",
    "outputId": "fe8e2212-37c8-4027-d176-ef46991bcd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu = True\n"
     ]
    }
   ],
   "source": [
    "use_gpu = torch.cuda.is_available()\n",
    "print(\"use_gpu = \" + str(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SYNSIGN Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "import os\n",
    "import csv\n",
    "import errno\n",
    "import scipy.io\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import google_drive\n",
    "\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading synsign.zip\n",
      "Extracting synsign.zip\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "trainset_syn = SYNSIGN(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_syn = torch.utils.data.DataLoader(trainset_syn, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "The 34th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(5)\n"
     ]
    }
   ],
   "source": [
    "# randomly plot a sample from training set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_syn):\n",
    "    inputs, labels = data\n",
    "    break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_syn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GTSRB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GTSRBv2.zip\n",
      "Extracting GTSRBv2.zip\n",
      "Done.\n",
      "Downloading GTSRBv2.zip\n",
      "Extracting GTSRBv2.zip\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "root_dir = \"data/\"\n",
    "\n",
    "transform_m = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "trainset_gtsrb = GTSRB(root_dir, train=True, transform=transform_m, download=True)\n",
    "trainloader_gtsrb = torch.utils.data.DataLoader(trainset_gtsrb, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)\n",
    "testset_gtsrb = GTSRB(root_dir, train=False, transform=transform_m, download=True)\n",
    "testloader_gtsrb = torch.utils.data.DataLoader(testset_gtsrb, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 52th image in the first 128 images in the training set:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Its correspondent label:\n",
      "tensor(31)\n"
     ]
    }
   ],
   "source": [
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_gtsrb):\n",
    "    inputs, labels = data\n",
    "    if i == 10:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\\n\" + str(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5AX_Msxj4we7"
   },
   "source": [
    "# Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 310,
     "status": "ok",
     "timestamp": 1523506395172,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "d5glpPlhprD4",
    "outputId": "6e9189d3-996c-434c-985e-0e88793fe4eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.C1 = nn.Conv2d(3, 96, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(96, 144, kernel_size=3, padding=2)\n",
    "        self.C3 = nn.Conv2d(144, 256, kernel_size=5, padding=2)\n",
    "        \n",
    "        \n",
    "        self.FC1 = nn.Linear(256 * 5 * 5, 512)\n",
    "        self.FC2 = nn.Linear(512, 43)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # M3\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # x's size is (128, 256, 5, 5)\n",
    "        # flatten\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        f = x\n",
    "        # FC1\n",
    "        x = F.relu(self.FC1(x))\n",
    "        # FC2\n",
    "        x = F.softmax(self.FC2(x))\n",
    "\n",
    "        return x, f, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(144, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "  (FC2): Linear(in_features=512, out_features=43, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_syn = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_syn.cuda()\n",
    "print(cnn_syn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            m.weight.data.normal_(0, sqrt(2 / n))\n",
    "            if m.bias is not None:\n",
    "                m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            size = m.weight.size()\n",
    "            fan_out = size[0] # number of rows\n",
    "            fan_in = size[1] # number of columns\n",
    "            m.weight.data.normal_(0, sqrt(2 / (fan_in + fan_out)))\n",
    "            m.bias.data.zero_()\n",
    "        elif hasattr(m, 'reset_parameters'):\n",
    "            m.reset_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pcSb16SIFgZt"
   },
   "source": [
    "# Training on SYNSIGN\n",
    "\n",
    "Or you can load the parameters directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model = False\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "para_file = \"./parameters/cnn_synsign\"\n",
    "load_model = os.path.isfile(para_file)\n",
    "print(\"load_model = \" + str(load_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip (not $load_model)\n",
    "#cnn_syn.load_state_dict(torch.load(para_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "YRHBSIQ2FkOL"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_syn.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DgUSfM0KF4iT"
   },
   "source": [
    "## Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(model, dataloader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs, _, _ = model(inputs)\n",
    "        correct += (torch.max(outputs.data, 1)[1] == labels.data).sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc = correct * 1.0 / total\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 10050
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 606703,
     "status": "ok",
     "timestamp": 1523144380194,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "IlstjDLbFkQ6",
    "outputId": "59a56d48-bdaa-4e0e-bc20-0feaecd74fa5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 300] batch loss: 3.666\n",
      "[ 600] batch loss: 3.515\n",
      "epoch 1 loss: inf -> 2788.595\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.24861441013460014\n",
      "[ 300] batch loss: 3.461\n",
      "[ 600] batch loss: 3.446\n",
      "epoch 2 loss: 2788.595 -> 2692.186\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.3341250989707047\n",
      "[ 300] batch loss: 3.370\n",
      "[ 600] batch loss: 3.351\n",
      "epoch 3 loss: 2692.186 -> 2619.534\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.33737133808392716\n",
      "[ 300] batch loss: 3.295\n",
      "[ 600] batch loss: 3.290\n",
      "epoch 4 loss: 2619.534 -> 2573.693\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.3477434679334917\n",
      "[ 300] batch loss: 3.286\n",
      "[ 600] batch loss: 3.258\n",
      "epoch 5 loss: 2573.693 -> 2553.243\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.3596991290577989\n",
      "[ 300] batch loss: 3.240\n",
      "[ 600] batch loss: 3.239\n",
      "epoch 6 loss: 2553.243 -> 2529.297\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.40190023752969123\n",
      "[ 300] batch loss: 3.208\n",
      "[ 600] batch loss: 3.195\n",
      "epoch 7 loss: 2529.297 -> 2498.160\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.43143309580364214\n",
      "[ 300] batch loss: 3.169\n",
      "[ 600] batch loss: 3.171\n",
      "epoch 8 loss: 2498.160 -> 2468.699\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.4323040380047506\n",
      "[ 300] batch loss: 3.106\n",
      "[ 600] batch loss: 3.107\n",
      "epoch 9 loss: 2468.699 -> 2427.981\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.4474267616785432\n",
      "[ 300] batch loss: 3.105\n",
      "[ 600] batch loss: 3.106\n",
      "epoch 10 loss: 2427.981 -> 2427.013\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.47973079968329374\n",
      "[ 300] batch loss: 3.104\n",
      "[ 600] batch loss: 3.105\n",
      "epoch 11 loss: 2427.013 -> 2426.556\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.48368962787015046\n",
      "[ 300] batch loss: 3.104\n",
      "[ 600] batch loss: 3.105\n",
      "epoch 12 loss: 2426.556 -> 2426.257\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.48250197941409345\n",
      "[ 300] batch loss: 3.103\n",
      "[ 600] batch loss: 3.105\n",
      "epoch 13 loss: 2426.257 -> 2426.018\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.478701504354711\n",
      "[ 300] batch loss: 3.103\n",
      "[ 600] batch loss: 3.083\n",
      "epoch 14 loss: 2426.018 -> 2415.382\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.4690419635787807\n",
      "[ 300] batch loss: 3.082\n",
      "[ 600] batch loss: 3.082\n",
      "epoch 15 loss: 2415.382 -> 2407.608\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.4776722090261283\n",
      "[ 300] batch loss: 3.059\n",
      "[ 600] batch loss: 3.059\n",
      "epoch 16 loss: 2407.608 -> 2391.178\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.4929532858273951\n",
      "[ 300] batch loss: 3.058\n",
      "[ 600] batch loss: 3.058\n",
      "epoch 17 loss: 2391.178 -> 2390.409\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.49897070467141724\n",
      "[ 300] batch loss: 3.057\n",
      "[ 600] batch loss: 3.058\n",
      "epoch 18 loss: 2390.409 -> 2390.096\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.4942992874109264\n",
      "[ 300] batch loss: 3.057\n",
      "[ 600] batch loss: 3.057\n",
      "epoch 19 loss: 2390.096 -> 2389.962\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.4980997624703088\n",
      "[ 300] batch loss: 3.057\n",
      "[ 600] batch loss: 3.057\n",
      "epoch 20 loss: 2389.962 -> 2389.854\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.494695170229612\n",
      "[ 300] batch loss: 3.057\n",
      "[ 600] batch loss: 3.049\n",
      "epoch 21 loss: 2389.854 -> 2383.361\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5050673000791766\n",
      "[ 300] batch loss: 3.034\n",
      "[ 600] batch loss: 3.031\n",
      "epoch 22 loss: 2383.361 -> 2366.759\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5298495645288994\n",
      "[ 300] batch loss: 3.010\n",
      "[ 600] batch loss: 3.010\n",
      "epoch 23 loss: 2366.759 -> 2352.811\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5213776722090261\n",
      "[ 300] batch loss: 3.010\n",
      "[ 600] batch loss: 3.009\n",
      "epoch 24 loss: 2352.811 -> 2349.250\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5303246239113223\n",
      "[ 300] batch loss: 2.986\n",
      "[ 600] batch loss: 2.986\n",
      "epoch 25 loss: 2349.250 -> 2334.747\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5448931116389548\n",
      "[ 300] batch loss: 2.986\n",
      "[ 600] batch loss: 2.986\n",
      "epoch 26 loss: 2334.747 -> 2334.609\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5414885193982582\n",
      "[ 300] batch loss: 2.986\n",
      "[ 600] batch loss: 2.986\n",
      "epoch 27 loss: 2334.609 -> 2334.577\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5350752177355502\n",
      "[ 300] batch loss: 2.986\n",
      "[ 600] batch loss: 2.986\n",
      "epoch 28 loss: 2334.577 -> 2334.510\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5449722882026921\n",
      "[ 300] batch loss: 2.986\n",
      "[ 600] batch loss: 2.986\n",
      "epoch 29 loss: 2334.510 -> 2334.453\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5482185273159145\n",
      "[ 300] batch loss: 2.986\n",
      "[ 600] batch loss: 2.971\n",
      "epoch 30 loss: 2334.453 -> 2325.896\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5417260490894695\n",
      "[ 300] batch loss: 2.963\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 31 loss: 2325.896 -> 2316.443\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5449722882026921\n",
      "[ 300] batch loss: 2.963\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 32 loss: 2316.443 -> 2316.381\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.548535233570863\n",
      "[ 300] batch loss: 2.963\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 33 loss: 2316.381 -> 2316.350\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5501979414093429\n",
      "[ 300] batch loss: 2.963\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 34 loss: 2316.350 -> 2316.331\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5488519398258116\n",
      "[ 300] batch loss: 2.963\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 35 loss: 2316.331 -> 2316.285\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.554473475851148\n",
      "[ 300] batch loss: 2.963\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 36 loss: 2316.285 -> 2316.263\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5532066508313539\n",
      "[ 300] batch loss: 2.963\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 37 loss: 2316.263 -> 2316.247\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5547901821060965\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 38 loss: 2316.247 -> 2316.226\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5547110055423594\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 39 loss: 2316.226 -> 2316.212\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5551068883610452\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 40 loss: 2316.212 -> 2316.196\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5574821852731592\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 41 loss: 2316.196 -> 2316.171\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5639746634996041\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 42 loss: 2316.171 -> 2316.152\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5631828978622327\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 43 loss: 2316.152 -> 2316.133\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5631828978622327\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 44 loss: 2316.133 -> 2316.130\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5644497228820269\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 45 loss: 2316.130 -> 2316.124\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5604117181314331\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 46 loss: 2316.124 -> 2316.119\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5606492478226445\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.963\n",
      "epoch 47 loss: 2316.119 -> 2316.109\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5623911322248615\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 48 loss: 2316.109 -> 2316.104\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5629453681710214\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 49 loss: 2316.104 -> 2316.099\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5652414885193983\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 50 loss: 2316.099 -> 2316.096\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5626286619160729\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 51 loss: 2316.096 -> 2316.082\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5639746634996041\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 52 loss: 2316.082 -> 2316.089\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5624703087885986\n",
      "[ 300] batch loss: 2.962\n",
      "[ 600] batch loss: 2.962\n",
      "epoch 53 loss: 2316.089 -> 2316.086\n",
      "\n",
      "Accuracy on GTSRB test set (source only): 0.5605700712589073\n",
      "[ 300] batch loss: 2.962\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-5db232e111a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0muse_gpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%skip $load_model\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 1000\n",
    "reset(cnn_syn)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_syn):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_syn(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 300 == 299:    # print every 300 mini-batches\n",
    "            print('[%4d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 300))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    print(\"Accuracy on GTSRB test set (source only): \" + str(evaluate_accuracy(cnn_syn, testloader_gtsrb)))\n",
    "    if prev_loss - epoch_loss < 0.001:\n",
    "        prev_loss = epoch_loss\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip $load_model\n",
    "\n",
    "torch.save(cnn_syn.state_dict(), para_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oBeLwnRLrcY"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing on source (no testing data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1632,
     "status": "ok",
     "timestamp": 1523144440407,
     "user": {
      "displayName": "Yuan Qi",
      "photoUrl": "//lh6.googleusercontent.com/--bd6SE8_hDo/AAAAAAAAAAI/AAAAAAAAAE0/J27oawL5omk/s50-c-k-no/photo.jpg",
      "userId": "112219197582513023329"
     },
     "user_tz": 420
    },
    "id": "Rn4sOe0iLuKI",
    "outputId": "f31ce94e-ef18-4564-9772-a3a9b22c1272"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on SYNSIGN train set: 0.83642\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on SYNSIGN train set: \" + str(evaluate_accuracy(cnn_syn, trainloader_syn)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GTSTB test set: 0.557403008709422\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on GTSTB test set: \" + str(evaluate_accuracy(cnn_syn, testloader_gtsrb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training on GTSRB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (C1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(144, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "  (FC2): Linear(in_features=512, out_features=43, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_gtsrb = CNN()\n",
    "if (use_gpu):\n",
    "    cnn_gtsrb.cuda()\n",
    "print(cnn_gtsrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_gtsrb = True\n"
     ]
    }
   ],
   "source": [
    "para_file_gtsrb = \"./parameters/cnn_gtsrb\"\n",
    "load_model_gtsrb = os.path.isfile(para_file_gtsrb)\n",
    "print(\"load_model_gtsrb = \" + str(load_model_gtsrb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip (not $load_model_gtsrb)\n",
    "#cnn_gtsrb.load_state_dict(torch.load(para_file_gtsrb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "lr_init = 0.1\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(cnn_gtsrb.parameters(), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9030\n"
     ]
    }
   ],
   "source": [
    "print(len(trainset_gtsrb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 50] batch loss: 3.761\n",
      "epoch 1 loss: inf -> 266.927\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.04117181314330958\n",
      "[ 50] batch loss: 3.756\n",
      "epoch 2 loss: 266.927 -> 266.534\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.014251781472684086\n",
      "[ 50] batch loss: 3.738\n",
      "epoch 3 loss: 266.534 -> 265.124\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.04053840063341251\n",
      "[ 50] batch loss: 3.694\n",
      "epoch 4 loss: 265.124 -> 261.507\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.037925574030087096\n",
      "[ 50] batch loss: 3.651\n",
      "epoch 5 loss: 261.507 -> 257.974\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.10245447347585115\n",
      "[ 50] batch loss: 3.587\n",
      "epoch 6 loss: 257.974 -> 253.146\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.14441805225653206\n",
      "[ 50] batch loss: 3.492\n",
      "epoch 7 loss: 253.146 -> 246.950\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.21931908155186064\n",
      "[ 50] batch loss: 3.460\n",
      "epoch 8 loss: 246.950 -> 245.295\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.2105304829770388\n",
      "[ 50] batch loss: 3.444\n",
      "epoch 9 loss: 245.295 -> 243.644\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.25558194774346793\n",
      "[ 50] batch loss: 3.413\n",
      "epoch 10 loss: 243.644 -> 241.732\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.25463182897862235\n",
      "[ 50] batch loss: 3.417\n",
      "epoch 11 loss: 241.732 -> 241.853\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.27505938242280287\n",
      "[ 50] batch loss: 3.393\n",
      "epoch 12 loss: 241.853 -> 240.059\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.29588281868566907\n",
      "[ 50] batch loss: 3.373\n",
      "epoch 13 loss: 240.059 -> 238.816\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.28297703879651626\n",
      "[ 50] batch loss: 3.370\n",
      "epoch 14 loss: 238.816 -> 238.752\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.28582739509105304\n",
      "[ 50] batch loss: 3.367\n",
      "epoch 15 loss: 238.752 -> 238.358\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.30712589073634206\n",
      "[ 50] batch loss: 3.360\n",
      "epoch 16 loss: 238.358 -> 237.963\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3022961203483769\n",
      "[ 50] batch loss: 3.361\n",
      "epoch 17 loss: 237.963 -> 238.058\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.28812351543942993\n",
      "[ 50] batch loss: 3.340\n",
      "epoch 18 loss: 238.058 -> 236.302\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3391132224861441\n",
      "[ 50] batch loss: 3.310\n",
      "epoch 19 loss: 236.302 -> 234.581\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3389548693586698\n",
      "[ 50] batch loss: 3.305\n",
      "epoch 20 loss: 234.581 -> 234.248\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3498812351543943\n",
      "[ 50] batch loss: 3.305\n",
      "epoch 21 loss: 234.248 -> 234.230\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3564528899445764\n",
      "[ 50] batch loss: 3.304\n",
      "epoch 22 loss: 234.230 -> 234.191\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3700712589073634\n",
      "[ 50] batch loss: 3.303\n",
      "epoch 23 loss: 234.191 -> 234.117\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3601741884402217\n",
      "[ 50] batch loss: 3.303\n",
      "epoch 24 loss: 234.117 -> 234.078\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.36634996041171813\n",
      "[ 50] batch loss: 3.285\n",
      "epoch 25 loss: 234.078 -> 232.366\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.3833729216152019\n",
      "[ 50] batch loss: 3.257\n",
      "epoch 26 loss: 232.366 -> 230.803\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.40031670625494853\n",
      "[ 50] batch loss: 3.249\n",
      "epoch 27 loss: 230.803 -> 230.020\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4145684877276326\n",
      "[ 50] batch loss: 3.236\n",
      "epoch 28 loss: 230.020 -> 229.346\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4173396674584323\n",
      "[ 50] batch loss: 3.235\n",
      "epoch 29 loss: 229.346 -> 229.250\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4210609659540776\n",
      "[ 50] batch loss: 3.233\n",
      "epoch 30 loss: 229.250 -> 229.167\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.41773555027711795\n",
      "[ 50] batch loss: 3.233\n",
      "epoch 31 loss: 229.167 -> 229.124\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4205859065716548\n",
      "[ 50] batch loss: 3.232\n",
      "epoch 32 loss: 229.124 -> 229.094\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.420110847189232\n",
      "[ 50] batch loss: 3.232\n",
      "epoch 33 loss: 229.094 -> 229.075\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4247822644497229\n",
      "[ 50] batch loss: 3.230\n",
      "epoch 34 loss: 229.075 -> 228.673\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4252573238321457\n",
      "[ 50] batch loss: 3.214\n",
      "epoch 35 loss: 228.673 -> 227.780\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4243072050673001\n",
      "[ 50] batch loss: 3.212\n",
      "epoch 36 loss: 227.780 -> 227.687\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.42636579572446553\n",
      "[ 50] batch loss: 3.211\n",
      "epoch 37 loss: 227.687 -> 227.631\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.428978622327791\n",
      "[ 50] batch loss: 3.211\n",
      "epoch 38 loss: 227.631 -> 227.551\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.43555027711797306\n",
      "[ 50] batch loss: 3.203\n",
      "epoch 39 loss: 227.551 -> 226.979\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.43642121931908157\n",
      "[ 50] batch loss: 3.202\n",
      "epoch 40 loss: 226.979 -> 226.918\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.438479809976247\n",
      "[ 50] batch loss: 3.199\n",
      "epoch 41 loss: 226.918 -> 226.749\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.42193190815518605\n",
      "[ 50] batch loss: 3.185\n",
      "epoch 42 loss: 226.749 -> 225.330\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4399049881235154\n",
      "[ 50] batch loss: 3.148\n",
      "epoch 43 loss: 225.330 -> 222.796\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4467933491686461\n",
      "[ 50] batch loss: 3.135\n",
      "epoch 44 loss: 222.796 -> 222.114\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.45447347585114806\n",
      "[ 50] batch loss: 3.131\n",
      "epoch 45 loss: 222.114 -> 221.866\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4701504354711006\n",
      "[ 50] batch loss: 3.117\n",
      "epoch 46 loss: 221.866 -> 220.711\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4714172604908947\n",
      "[ 50] batch loss: 3.107\n",
      "epoch 47 loss: 220.711 -> 220.161\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4653206650831354\n",
      "[ 50] batch loss: 3.106\n",
      "epoch 48 loss: 220.161 -> 220.138\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4761678543151227\n",
      "[ 50] batch loss: 3.106\n",
      "epoch 49 loss: 220.138 -> 220.122\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.47339667458432305\n",
      "[ 50] batch loss: 3.106\n",
      "epoch 50 loss: 220.122 -> 220.012\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4714172604908947\n",
      "[ 50] batch loss: 3.087\n",
      "epoch 51 loss: 220.012 -> 218.767\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.495882818685669\n",
      "[ 50] batch loss: 3.083\n",
      "epoch 52 loss: 218.767 -> 218.573\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4646080760095012\n",
      "[ 50] batch loss: 3.085\n",
      "epoch 53 loss: 218.573 -> 218.602\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4916072842438638\n",
      "[ 50] batch loss: 3.083\n",
      "epoch 54 loss: 218.602 -> 218.484\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4766429136975455\n",
      "[ 50] batch loss: 3.081\n",
      "epoch 55 loss: 218.484 -> 218.304\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.49707046714172604\n",
      "[ 50] batch loss: 3.062\n",
      "epoch 56 loss: 218.304 -> 217.031\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.4983372921615202\n",
      "[ 50] batch loss: 3.059\n",
      "epoch 57 loss: 217.031 -> 216.881\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5293745051464767\n",
      "[ 50] batch loss: 3.059\n",
      "epoch 58 loss: 216.881 -> 216.836\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5326207442596991\n",
      "[ 50] batch loss: 3.058\n",
      "epoch 59 loss: 216.836 -> 216.552\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5234362628661916\n",
      "[ 50] batch loss: 3.037\n",
      "epoch 60 loss: 216.552 -> 215.350\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5315122723673793\n",
      "[ 50] batch loss: 3.036\n",
      "epoch 61 loss: 215.350 -> 215.249\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5391132224861441\n",
      "[ 50] batch loss: 3.035\n",
      "epoch 62 loss: 215.249 -> 215.222\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5484560570071259\n",
      "[ 50] batch loss: 3.035\n",
      "epoch 63 loss: 215.222 -> 215.202\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5513064133016627\n",
      "[ 50] batch loss: 3.035\n",
      "epoch 64 loss: 215.202 -> 215.193\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5517814726840855\n",
      "[ 50] batch loss: 3.035\n",
      "epoch 65 loss: 215.193 -> 215.189\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5513064133016627\n",
      "[ 50] batch loss: 3.035\n",
      "epoch 66 loss: 215.189 -> 215.186\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.55027711797308\n",
      "[ 50] batch loss: 3.035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 loss: 215.186 -> 215.182\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.55027711797308\n",
      "[ 50] batch loss: 3.033\n",
      "epoch 68 loss: 215.182 -> 215.028\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5482185273159145\n",
      "[ 50] batch loss: 3.019\n",
      "epoch 69 loss: 215.028 -> 214.001\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5505146476642914\n",
      "[ 50] batch loss: 3.014\n",
      "epoch 70 loss: 214.001 -> 213.645\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5627870150435471\n",
      "[ 50] batch loss: 3.013\n",
      "epoch 71 loss: 213.645 -> 213.590\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5585114806017419\n",
      "[ 50] batch loss: 3.012\n",
      "epoch 72 loss: 213.590 -> 213.572\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.561520190023753\n",
      "[ 50] batch loss: 3.012\n",
      "epoch 73 loss: 213.572 -> 213.564\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5647664291369755\n",
      "[ 50] batch loss: 2.999\n",
      "epoch 74 loss: 213.564 -> 212.470\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5505146476642914\n",
      "[ 50] batch loss: 2.991\n",
      "epoch 75 loss: 212.470 -> 212.015\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5655581947743468\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 76 loss: 212.015 -> 211.934\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5695961995249407\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 77 loss: 211.934 -> 211.927\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5679334916864608\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 78 loss: 211.927 -> 211.922\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5693586698337292\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 79 loss: 211.922 -> 211.920\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5689627870150435\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 80 loss: 211.920 -> 211.919\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5687252573238322\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 81 loss: 211.919 -> 211.918\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5716547901821061\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 82 loss: 211.918 -> 211.914\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5707838479809976\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 83 loss: 211.914 -> 211.911\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5726049089469517\n",
      "[ 50] batch loss: 2.989\n",
      "epoch 84 loss: 211.911 -> 211.908\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5732383214568487\n",
      "[ 50] batch loss: 2.988\n",
      "epoch 85 loss: 211.908 -> 211.714\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5494061757719715\n",
      "[ 50] batch loss: 2.973\n",
      "epoch 86 loss: 211.714 -> 210.676\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5464766429136976\n",
      "[ 50] batch loss: 2.968\n",
      "epoch 87 loss: 210.676 -> 210.435\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5493269992082344\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 88 loss: 210.435 -> 210.315\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5549485352335709\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 89 loss: 210.315 -> 210.301\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5585906571654791\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 90 loss: 210.301 -> 210.295\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5604908946951702\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 91 loss: 210.295 -> 210.291\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5622327790973872\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 92 loss: 210.291 -> 210.291\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5615993665874901\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 93 loss: 210.291 -> 210.291\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5668250197941409\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 94 loss: 210.291 -> 210.280\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5656373713380839\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 95 loss: 210.280 -> 210.276\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5665083135391924\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 96 loss: 210.276 -> 210.275\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5658749010292953\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 97 loss: 210.275 -> 210.274\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5680126682501979\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 98 loss: 210.274 -> 210.272\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5676959619952494\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 99 loss: 210.272 -> 210.272\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5675376088677752\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 100 loss: 210.272 -> 210.271\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5663499604117181\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 101 loss: 210.271 -> 210.271\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5651623119556611\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 102 loss: 210.271 -> 210.270\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5650039588281869\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 103 loss: 210.270 -> 210.269\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5648456057007126\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 104 loss: 210.269 -> 210.268\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5665083135391924\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 105 loss: 210.268 -> 210.268\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5668250197941409\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 106 loss: 210.268 -> 210.268\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5673792557403009\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 107 loss: 210.268 -> 210.268\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5665874901029295\n",
      "[ 50] batch loss: 2.966\n",
      "epoch 108 loss: 210.268 -> 210.266\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5676167854315123\n",
      "[ 50] batch loss: 2.961\n",
      "epoch 109 loss: 210.266 -> 209.722\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5594615993665875\n",
      "[ 50] batch loss: 2.944\n",
      "epoch 110 loss: 209.722 -> 208.717\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5623911322248615\n",
      "[ 50] batch loss: 2.943\n",
      "epoch 111 loss: 208.717 -> 208.681\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5746634996041172\n",
      "[ 50] batch loss: 2.943\n",
      "epoch 112 loss: 208.681 -> 208.651\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5726049089469517\n",
      "[ 50] batch loss: 2.943\n",
      "epoch 113 loss: 208.651 -> 208.644\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5737133808392716\n",
      "[ 50] batch loss: 2.943\n",
      "epoch 114 loss: 208.644 -> 208.642\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5747426761678543\n",
      "[ 50] batch loss: 2.943\n",
      "epoch 115 loss: 208.642 -> 208.640\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5744259699129057\n",
      "[ 50] batch loss: 2.943\n",
      "epoch 116 loss: 208.640 -> 208.638\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5741092636579572\n",
      "[ 50] batch loss: 2.936\n",
      "epoch 117 loss: 208.638 -> 207.934\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5642121931908155\n",
      "[ 50] batch loss: 2.920\n",
      "epoch 118 loss: 207.934 -> 207.098\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.566270783847981\n",
      "[ 50] batch loss: 2.919\n",
      "epoch 119 loss: 207.098 -> 207.034\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5690419635787807\n",
      "[ 50] batch loss: 2.919\n",
      "epoch 120 loss: 207.034 -> 207.026\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5694378463974663\n",
      "[ 50] batch loss: 2.919\n",
      "epoch 121 loss: 207.026 -> 207.023\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5703879651623119\n",
      "[ 50] batch loss: 2.919\n",
      "epoch 122 loss: 207.023 -> 207.020\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5707046714172604\n",
      "[ 50] batch loss: 2.918\n",
      "epoch 123 loss: 207.020 -> 206.800\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5353127474267617\n",
      "[ 50] batch loss: 2.898\n",
      "epoch 124 loss: 206.800 -> 205.510\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5661124307205068\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 125 loss: 205.510 -> 205.423\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5688836104513064\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 126 loss: 205.423 -> 205.416\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5771971496437055\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 127 loss: 205.416 -> 205.408\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.579334916864608\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 128 loss: 205.408 -> 205.402\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.576959619952494\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 129 loss: 205.402 -> 205.399\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5766429136975455\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 130 loss: 205.399 -> 205.397\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5775930324623911\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 131 loss: 205.397 -> 205.396\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5780680918448139\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 132 loss: 205.396 -> 205.394\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5779889152810768\n",
      "[ 50] batch loss: 2.896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 133 loss: 205.394 -> 205.393\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5779097387173396\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 134 loss: 205.393 -> 205.392\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5777513855898654\n",
      "[ 50] batch loss: 2.896\n",
      "epoch 135 loss: 205.392 -> 205.391\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5768804433887569\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 136 loss: 205.391 -> 205.390\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5771971496437055\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 137 loss: 205.390 -> 205.390\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5789390340459224\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 138 loss: 205.390 -> 205.389\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5783056215360254\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 139 loss: 205.389 -> 205.388\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5792557403008709\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 140 loss: 205.388 -> 205.387\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5796516231195566\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 141 loss: 205.387 -> 205.387\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5794932699920824\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 142 loss: 205.387 -> 205.386\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.580522565320665\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 143 loss: 205.386 -> 205.385\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5802850356294537\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 144 loss: 205.385 -> 205.384\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5801266825019794\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 145 loss: 205.384 -> 205.384\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5798099762470309\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 146 loss: 205.384 -> 205.384\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5796516231195566\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 147 loss: 205.384 -> 205.383\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5798099762470309\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 148 loss: 205.383 -> 205.383\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5797307996832938\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 149 loss: 205.383 -> 205.383\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5799683293745052\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 150 loss: 205.383 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5802058590657165\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 151 loss: 205.382 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5802058590657165\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 152 loss: 205.382 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.579889152810768\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 153 loss: 205.382 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5797307996832938\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 154 loss: 205.382 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5796516231195566\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 155 loss: 205.382 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5794932699920824\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 156 loss: 205.381 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5791765637371338\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 157 loss: 205.382 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5795724465558195\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 158 loss: 205.382 -> 205.382\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.579889152810768\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 159 loss: 205.382 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5796516231195566\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 160 loss: 205.381 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5796516231195566\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 161 loss: 205.381 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5801266825019794\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 162 loss: 205.381 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5795724465558195\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 163 loss: 205.381 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5792557403008709\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 164 loss: 205.381 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.5792557403008709\n",
      "[ 50] batch loss: 2.895\n",
      "epoch 165 loss: 205.381 -> 205.381\n",
      "\n",
      "Accuracy on GTSTB test set (train on target): 0.579334916864608\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-92702e0cbd41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m49\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# print every 50 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_gtsrb\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 1000\n",
    "reset(cnn_gtsrb)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    adjust_lr(optimizer, p)\n",
    "    for i, data in enumerate(trainloader_gtsrb):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs, _, _ = cnn_gtsrb(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 50 == 49:    # print every 50 mini-batches\n",
    "            print('[%3d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    print(\"Accuracy on GTSTB test set (train on target): \" + str(evaluate_accuracy(cnn_gtsrb, testloader_gtsrb)))\n",
    "    if prev_loss - epoch_loss < 0.001:\n",
    "        prev_loss = epoch_loss   \n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%skip $load_model_gtsrb\n",
    "\n",
    "torch.save(cnn_gtsrb.state_dict(), para_file_gtsrb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:38: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on GTSTB test set (train on target): 0.5789390340459224\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy on GTSTB test set (train on target): \" + str(evaluate_accuracy(cnn_gtsrb, testloader_gtsrb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Domain Adaptation\n",
    "\n",
    "## Join Source and Target Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_Dataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Source and target dataset combination.\"\"\"\n",
    "    \n",
    "    def __init__(self, source, target, batch_size):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source (torch.utils.data.Dataset): Source dataset.\n",
    "            target (torch.utils.data.Dataset): Target dataset.\n",
    "            batch_size (int): Batch size.\n",
    "        \"\"\"\n",
    "        small_len = min(len(source), len(target))\n",
    "        large_len = max(len(source), len(target))\n",
    "        \n",
    "        small_len = batch_size * (small_len // batch_size)\n",
    "        self.small_len = small_len\n",
    "        large_len = batch_size * (large_len // batch_size)\n",
    "        self.large_len = large_len\n",
    "        \n",
    "        self.length = small_len * (large_len // small_len) * 2\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.half_batch_size = batch_size // 2\n",
    "        \n",
    "        channel, height, width = source[0][0].shape\n",
    "        \n",
    "        \n",
    "        self.images = torch.Tensor(self.length, channel, height, width)\n",
    "        self.labels = torch.LongTensor(self.length)\n",
    "        self.domains = torch.LongTensor(self.length)\n",
    "\n",
    "        \n",
    "        for idx in range(self.length):\n",
    "            if idx // 64 % 2 == 0:\n",
    "                # source\n",
    "                idx_s = idx // 128 * 64 + idx % 128\n",
    "                self.images[idx] = torch.from_numpy(source[idx_s][0]).type(torch.FloatTensor)\n",
    "                self.labels[idx] = source[idx_s][1]\n",
    "                self.domains[idx] = 0\n",
    "            else:\n",
    "                # target\n",
    "                idx_t = (idx // 128 * 64 + idx % 128 - 64) % self.small_len\n",
    "                self.images[idx] = torch.from_numpy(target[idx_t][0]).type(torch.FloatTensor)\n",
    "                self.labels[idx] = -1 #target[idx_t][1]\n",
    "                self.domains[idx] = 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.labels[idx], self.domains[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "trainset_da = ST_Dataset(trainset_syn, trainset_gtsrb, batch_size)\n",
    "trainloader_da = torch.utils.data.DataLoader(trainset_da, batch_size=batch_size,\n",
    "                                          shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# randomly plot a sample from test set\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "inputs = None\n",
    "labels = None\n",
    "for i, data in enumerate(trainloader_da):\n",
    "    inputs, labels, domain = data\n",
    "    if i == 0:\n",
    "        break\n",
    "\n",
    "idx = np.random.randint(0, batch_size)\n",
    "print(\"The \" + str(idx) + \"th image in the first \" + str(batch_size) +\\\n",
    "      \" images in the training set:\")\n",
    "plt.figure(figsize=(1,1))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(inputs[idx].permute(1, 2, 0).numpy())\n",
    "plt.show()\n",
    "print(\"Its correspondent label:\")\n",
    "if labels[idx].item() == -1:\n",
    "    print(\"I don't know :)\\n\")\n",
    "else:\n",
    "    print(str(labels[idx].item()) + \"\\n\")\n",
    "\n",
    "print(\"From domain:\")\n",
    "if domain[idx].item() == 0:\n",
    "    print(\"Source\")\n",
    "else:\n",
    "    print(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GRL Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL_func(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, inputs, lamda):\n",
    "        ctx.save_for_backward(lamda)\n",
    "        return inputs\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_outputs):\n",
    "        lamda, = ctx.saved_tensors\n",
    "        return -lamda * grad_outputs, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRL(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init):\n",
    "        super(GRL, self).__init__()\n",
    "        self.GRL_func = GRL_func.apply\n",
    "        self.lamda = nn.Parameter(torch.Tensor(1), requires_grad=False)\n",
    "        self.set_lamda(lamda_init)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.GRL_func(x, self.lamda)\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.lamda[0] = lamda_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure\n",
    "\n",
    "![SVHN Structure](https://c1.staticflickr.com/1/907/41989310211_cb9d63bcc2_o.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_DA(nn.Module):\n",
    "    \n",
    "    def __init__(self, lamda_init=0):\n",
    "        super(CNN_DA, self).__init__()\n",
    "        # lamda\n",
    "        self.lamda = lamda_init\n",
    "        # feature extractor\n",
    "        self.C1 = nn.Conv2d(3, 96, kernel_size=5, padding=2)\n",
    "        self.C2 = nn.Conv2d(96, 144, kernel_size=3, padding=2)\n",
    "        self.C3 = nn.Conv2d(144, 256, kernel_size=5, padding=2)\n",
    "        # label classifier\n",
    "        self.FC1 = nn.Linear(256 * 5 * 5, 512)\n",
    "        self.FC2 = nn.Linear(512, 43)\n",
    "        # domain classifier\n",
    "        self.GRL_layer = GRL(lamda_init)\n",
    "        self.DC_FC1 = nn.Linear(256 * 5 * 5, 1024)\n",
    "        self.DC_FC2 = nn.Linear(1024, 1024)\n",
    "        self.DC_FC3 = nn.Linear(1024, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # C1\n",
    "        x = F.relu(self.C1(x))\n",
    "        # M1\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C2\n",
    "        x = F.relu(self.C2(x))\n",
    "        # M2\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # C3\n",
    "        x = F.relu(self.C3(x))\n",
    "        # M3\n",
    "        x = F.max_pool2d(x, (2, 2), stride=(2, 2))\n",
    "        # x's size is (128, 256, 5, 5)\n",
    "        # flatten\n",
    "        x = x.view(-1, 256 * 5 * 5)\n",
    "        f = x\n",
    "        # label classifier\n",
    "        # FC1\n",
    "        x_l = F.relu(self.FC1(x))\n",
    "        # FC2F\n",
    "        x_l = F.softmax(self.FC2(x_l))\n",
    "        # domain classifier\n",
    "        # GRL\n",
    "        x_d = self.GRL_layer(x)\n",
    "        # DC_FC1\n",
    "        x_d = F.relu(self.DC_FC1(x_d))\n",
    "        # DC_FC2\n",
    "        x_d = F.relu(self.DC_FC2(x_d))\n",
    "        lh = x\n",
    "        # DC_FC3\n",
    "        x_d = F.sigmoid(self.DC_FC3(x_d))\n",
    "        return x_l, x_d, f, lh\n",
    "    \n",
    "    def set_lamda(self, lamda_new):\n",
    "        self.GRL_layer.set_lamda(lamda_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_DA(\n",
      "  (C1): Conv2d(3, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (C2): Conv2d(96, 144, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
      "  (C3): Conv2d(144, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (FC1): Linear(in_features=6400, out_features=512, bias=True)\n",
      "  (FC2): Linear(in_features=512, out_features=43, bias=True)\n",
      "  (GRL_layer): GRL()\n",
      "  (DC_FC1): Linear(in_features=6400, out_features=1024, bias=True)\n",
      "  (DC_FC2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (DC_FC3): Linear(in_features=1024, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "cnn_da = CNN_DA(0)\n",
    "if (use_gpu):\n",
    "    cnn_da.cuda()\n",
    "print(cnn_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_model_da = False\n"
     ]
    }
   ],
   "source": [
    "para_file_da = \"./parameters/cnn_syn_to_gtsrp\"\n",
    "load_model_da = os.path.isfile(para_file_da)\n",
    "print(\"load_model_da = \" + str(load_model_da))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip (not $load_model_da)\n",
    "cnn.load_state_dict(torch.load(para_file_da))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "\n",
    "lr_init = 0.001\n",
    "criterion_LC = nn.CrossEntropyLoss()\n",
    "criterion_DC = nn.BCELoss()\n",
    "optimizer = optim.SGD(filter(lambda p: p.requires_grad, cnn_da.parameters()), lr=lr_init, momentum=0.9)\n",
    "\n",
    "def adjust_lr(optimizer, p):\n",
    "    global lr_init\n",
    "    lr_0 = lr_init\n",
    "    alpha = 10\n",
    "    beta = 0.75\n",
    "    lr = lr_0 / (1 + alpha * p) ** beta\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "    return lr\n",
    "        \n",
    "def adjust_lamda(model, p):\n",
    "    gamma = 10\n",
    "    lamda = 2 / (1 + exp(- gamma * p)) - 1\n",
    "    model.set_lamda(lamda)\n",
    "    return lamda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_da_accuracy(model, dataloader, source):\n",
    "    correct_LC = 0\n",
    "    correct_DC = 0\n",
    "    total = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs, labels = data\n",
    "        if (use_gpu):\n",
    "            inputs, labels = inputs.type(torch.FloatTensor).cuda(), labels.cuda()\n",
    "        inputs, labels = Variable(inputs), Variable(labels)\n",
    "        outputs_LC, outputs_DC, _, _ = model(inputs)\n",
    "        correct_LC += (torch.max(outputs_LC.data, 1)[1] == labels.data).sum().item()\n",
    "        if source:\n",
    "            correct_DC += labels.size()[0] - outputs_DC.data.sum().item()\n",
    "        else:\n",
    "            correct_DC += outputs_DC.data.sum().item()\n",
    "        total += labels.size()[0]\n",
    "    acc_LC = correct_LC / total\n",
    "    acc_DC = correct_DC / total\n",
    "    return acc_LC, acc_DC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 600] batch loss: 4.453\n",
      "[1200] batch loss: 4.452\n",
      "epoch 1 loss: inf -> 6856.462\n",
      "\n",
      "Label classifier accuracy on GTSRP test set (DA): 0.026287\n",
      "Domain classifier accuracy on GTSRP test set (DA): 0.485872\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-ecacc731e8a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m600\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m599\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# print every 600 mini-batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%skip $load_model_da\n",
    "\n",
    "prev_loss = np.float(\"inf\")\n",
    "total_epoch = 100\n",
    "reset(cnn_da)\n",
    "\n",
    "for epoch in range(total_epoch):\n",
    "    epoch_loss = 0.0\n",
    "    running_loss = 0.0\n",
    "    p = epoch * 1.0 / total_epoch\n",
    "    p=1\n",
    "    adjust_lr(optimizer, p)\n",
    "    lamda = adjust_lamda(cnn_da, p)\n",
    "    for i, data in enumerate(trainloader_da):\n",
    "        source_size = data[0].size()[0] // 2\n",
    "        inputs, labels, domains = data\n",
    "        domains = domains.to(torch.float32)\n",
    "        if (use_gpu):\n",
    "            inputs, labels, domains = inputs.type(torch.FloatTensor).cuda(), labels.cuda(), domains.cuda()\n",
    "        inputs, labels, domains = Variable(inputs), Variable(labels), Variable(domains)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_LC, outputs_DC, _, _ = cnn_da(inputs)\n",
    "        outputs_DC = outputs_DC.view(-1)\n",
    "        loss_LC = criterion_LC(outputs_LC[:source_size], labels[:source_size])\n",
    "        loss_DC = criterion_DC(outputs_DC, domains)\n",
    "        loss = loss_LC + loss_DC\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        running_loss += loss.item()\n",
    "        if i % 600 == 599:    # print every 600 mini-batches\n",
    "            print('[%4d] batch loss: %.3f' %\n",
    "                  (i + 1, running_loss / 600))\n",
    "            running_loss = 0.0\n",
    "    print(\"epoch %d loss: %.3f -> %.3f\\n\" % (epoch + 1, prev_loss, epoch_loss))\n",
    "    print((\"Label classifier accuracy on GTSRP test set (DA): %f\\n\"\n",
    "           \"Domain classifier accuracy on GTSRP test set (DA): %f\")\n",
    "          %evaluate_da_accuracy(cnn_da, testloader_gtsrb, source=False))\n",
    "    if prev_loss - epoch_loss < 0.001:\n",
    "        prev_loss = epoch_loss\n",
    "        pass\n",
    "    else:\n",
    "        prev_loss = epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%skip $load_model_da\n",
    "\n",
    "torch.save(cnn_da.state_dict(), para_file_da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print((\"Label classifier accuracy on SYNNUM test set (DA): %f\\n\"\n",
    "       \"Domain classifier accuracy on SYNNUM test set (DA): %f\")\n",
    "      %evaluate_da_accuracy(cnn_da, testloader_syn, source=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((\"Label classifier accuracy on SVHN test set (DA): %f\\n\"\n",
    "       \"Domain classifier accuracy on SVHN test set (DA): %f\")\n",
    "      %evaluate_da_accuracy(cnn_da, testloader_svhn, source=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "My_CNN_Tutorial.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
